[
{
		"title": "DF Definitions",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/df/df-definitions/",
		"content": "more definition in [[Ethereum and DeFi Glossary.pdf]]\npegged cryptocurrency: cryptocurrency whose value is linked to a specific bank-issued currency, financial instrument or tradable commodity\n\ne.g. pay back to buy BTC and holds it for you\n\nKnow your customer ( #short KYC): means verifying who you are when joining a crypto exchange\nAnti-money laundering ( #short AML): helping to break criminal networks and minimise the impact of illicit transactions on affected economies.\nwashtrading: occurs when an investor buys and sells the same or a similar security investment at the same time (e.g. can be done by using flash loans)\n(Fractional) reserve: bank's reserve ratio = (money held by bank) / (money deposited by bank customers)\nExchange traded fund ( #short ETF): security/asset\n\ntracking an index/sector/commodity\ntradable on an exchange\ntypically, lower fees than buying individual stocks\nEFT types: bond, stock, industry, commodity, currency, inverse, etc\ne.g. SPDR S&amp;P 500 ETF tracks the S&amp;P 500 index\n\ndecentralised autonomous organisation ( #short DAO): e.g. <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/df/df-l6/#maker-dao-pasted-image-20240405232329-png\">MakerDAO</a>\n\nLending\n\ncollateral: assets that serve as a security deposit\nOver-collateralisation: Borrower has to provide value(collateral assets) &gt; value(granted loan)\nUnder-collateralisation: value(collateral) &lt; value(debt)\nLiquidation: selling collateral from the borrower\n\nE.g., if value(collateral) &lt;= 150% x value(debt)\nAnyone can liquidate the debt position\n\nHealth factor\n\n0 &lt; liquidation threshold &lt; 1\nliquidation threshold provides a &quot;secure&quot; margin\nwhen health factor &lt; 1 =&gt; borrowing position becomes liquidatable\ne.g.\n\nLiquidation Spread #short LS: bonus, or discount, that a liquidator can collect when liquidating collateral\n\nğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ ğ‘œğ‘“ ğ¶ğ‘œğ‘™ğ‘™ğ‘ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘™ ğ‘¡ğ‘œ ğ¶ğ‘™ğ‘ğ‘–ğ‘š = ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ ğ‘œğ‘“ ğ·ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ ğ‘…ğ‘’ğ‘ğ‘ğ‘¦ Ã— (1 + ğ¿ğ‘†)\n\nClose Factor #short CF: the maximum proportion of the debt that is allowed to be repaid in a single fixed spread liquidation\nğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ ğ‘œğ‘“ ğ·ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ ğ‘…ğ‘’ğ‘ğ‘ğ‘¦ &lt; ğ¶ğ¹ Ã— ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ ğ‘œğ‘“ ğ·ğ‘’ğ‘ğ‘¡ğ‘ ",
		"tags": ["short", "short", "short", "short", "short", "short", "note"]
},

{
		"title": "DF L1",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/df/df-l1/",
		"content": "#disclaimer Not examrelevant!\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/df/financial-markets/\">Financial Markets</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/df/decentralised-finance/\">Decentralised Finance</a>",
		"tags": ["disclaimer", "note"]
},

{
		"title": "DF L2",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/df/df-l2/",
		"content": "&lt;![[Ethereum and DeFi Glossary.pdf]]\nQuiz\n\nA valid signed transaction of an account can be used to computer\n\nthe private key of the account\nthe address of the account [true]\n(both are true)\n(both are false)\n\nRunning a node in the Ethereum network\n\nwill earn you money\nwill cost you proof-of-work mining energy\nneeds an excellent internet connection (because you constantly need to be connected to 5'000 other nodes)\n(none of the above) [true]\n\nAn Ethereum smart contract can be programmed to\n\ninteract with a normal (not Ethereum) computer program through remote procedure calls.\nautomatically run every hour.\n(both are ture)\n(both are false) [true]\n\nLayer 2 protocols\n\nare not realy (only exist in reseach)\nare considered illegal (by the Ethereum foundation)\nhave been co-invented at ETH Zurich [true]\n(none of the above)\n\nIgnore fees, how many bananas do you get when you send 10 apples to a CPMM that holds a liquidity of 30 apples and 56 bananas?\n\nHow to calculate?\n\nconstant product = 30 [apples] * 56 [bananas] = 168\nand afterwards there has to be the same again thus 40 (30 + 10 new) [apples] * x [bananas] = 168\nsolve for x\n\nalternative: calculate ratio after adding 56 [bananas] / 40 [apples] = 7 / 5 =&gt; now multiply with number of apples and you get bananas 7 [bananas] / 5 [apples] * 10 [apples] = 14 [bananas]",
		"tags": [ "note"]
},

{
		"title": "DF L3",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/df/df-l3/",
		"content": "DF and CF\nDF stack\n\nExample where smart contract on blockchain uses external data\n\nOracle\n\nReasons for need of oracles for blockchains\n\nblockchain is isolated DB\nblockchains lack\n\naccess to real-world data\nno API-query possible\ncannot browse the internet\n\nSolution: transactions\nDefinition\n\nGeneral: System that connects a blockchain with other systems.\nSpecific: Actors relaying data on-chain.\n\nDesign\nChallenges\n\nminers (provider of data) can lie/cheat\nnetwork providing data for miners can lie/cheat\nsource or oracle outage\n=&gt; have multiple miners and sources to prevent outage\n=&gt; have consensus so possibility of false data is minimised\n\nCensorship\n\ncan happen on multiple layers e.g. application, transaction, consensus\n\nMixers\n\nbreaks connection to person who put it in =&gt; but observations/analysis of transaction can help with finding out who gave/took coins\n(in best case) just fixed values e.g. 1 ETH possible, so less\n\nImplications\n\nConfirmation latency: slows down transaction confirmations\nDenial of Service: A lot of (fake) transactions can slow node down or even completely take it down\n\nproposer/builder separation\nattack nodes who apply censorship\n\nfor censorship, nodes have to put in work to extract information but then cannot do transactions and don't earn anything bc. transaction with a censored value is not allowed =&gt; DDoS attack possible to spam node with censored coins\n\nQuiz\n\nWhich of the following DeFi applications is most likely to require external oracle data?\n\nToken management contracts, such as ERC20\nBetting on real world events [correct]\nUniswap, Curve, and other Automated Market Maker Decentralised Exchanges\nOn-chain games that do not require randomness\n\nIs the context of blockchain oracles, which of the following statements is NOT TRUE?\n\nBecause block generation is centralised process, incorporating an oracle into the consensus protocol does not work because miners may lie or cheat.\nOracles have to receive very concise and short data representations to minimise blockchain fees.\nFor each oracle update, a minimum subset of all nodes in the oracle committee should send a report to the miner. [correct bc. one node is enough]\nMultiple sources /website (redundancy) are required due to website outages and data corruption possibilities\n\nWhat is the anonymity set of Tronado Cach pool with 1000 deposits\n\n100\n1000 [correct]\n999\n1001\n\nWhat are the computational implications if a miner censors a transaction, what is correct?\n\nNo implication bc. the miner doesn't need to verify the transaction.\nNo implications bc. the miner doesn't need to execute the transaction.\nThe minder must spend computational resources to execute the transaction but cannot claim transaction fees. [correct]\nThe miner must spend computational resources but is paid the regular transaction fees.\n\nAssumed an oracle committee has five nodes (A,B,C,D,E) and it's agreed to report the median value. Node E is a malicious node and E observes the following values from the other four nodes (A:1000, B: 1010, C: 1020, D: 1030). What are the upper and lower bounds of the aggregated value to which E can manipulate.\n\n1000 - 1005\n1010-1020 [correct bc. median is middle number and you can choose a number between (including borders) 1010 to 1020 and it will be chosen]\n1000 - 1030\nno upper/lower bounds",
		"tags": [ "note"]
},

{
		"title": "DF L4",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/df/df-l4/",
		"content": "Decentralised Exchange\n(Example LOB Dex)\n\n(Dis) advantages:\n\n+ No KYC/AML\n+ no fees paid to the exchange\n+ no impermanent loss\n- fees for deposit, withdraw, trade creation/cancel\n- slow execution\n- not fully decentralised (mediating server)\n\ntrading volume: around 70 billion (recent months)\nWhy?\n\nSystems Architecture\nAutomated market maker\n#short AMM\n\nLiquidity pool idea: Let a smart contract do the market making. liquidity provider (LP) token:\nformula xâˆ—y=k where x = asset X quantity, y = asset Y quantity, k = constant\nproperties\n\ninstant liquidity, irrespective of the trade size\npurchase of asset X increases price of X and decreases the price of Y\nratio of asset X and Y sets the price\nknown as constant product ( #short CP) AMM\n\n(dis)advantages\n\nno order book maintenance: but arbitrage required\n\nsimple implementation for CP AMM: low gas costs\n\ndanger of impermanent loss/coin de-peg: total loss of funds possible\n\nhigh slippage for low liquidity markets\n\nusers vulnerable to sandwich attacks\n\nExample:\n\nSlippage\n\n(un)expected increase or decrease in price based on trading volume and available liquidity\neffects\n\nworse/better execution price\n\npossible to introduce a slippage protection by configuring a threshold to prevent unacceptable slippage\n\nImpermanent loss\n#short IL\n\nimpermanent == not permanet: realised upon withdraw only\ncan result in total loss\n\ntrading fees may compensate\nliquidity mining may compensate\nsimilar to a de-peg of stablecoin\n\nExchange Transaction propagation\nStablecoin AMM pros/cons\n\nbetter prices for bigger volumes i.e. more liquidity\n\nexample shows significant differences among exchanges\n\npotentially higher gas costs\n\ndanger of a de-peg of a stablecoin\n\npegged/stablecoin prices move in expectation together\n\nexchange rate should ideally remain 1:1\ndefault CP AMM is not optimised for such cases\n\nWhat if a coin gets\n\nde-pug\n\nas bank run in CeFi: no money returned\n&quot;bank run&quot; in DeFi\n\npool may get blacklisted\nfirst come, first served\npool formula penalises destabilising a pool\nincreasingly worse price for late-comers\n\nblacklisted\n\ne.g. USDT and USDC have built-in code to\n\nblacklist addresses\ndestroy coins\ne.g. USDT blacklisted 500+ accounts, destroyed 50M+ USDT\n\nAMM arbitrage\n\nbeginning: multiple markets with\n\nthe same assets X and Y\ndifferent prices for X and Y\n\nprices are synchronised by &quot;arbitrageurs&quot;\n\nthey gain profit from price difference: also referred to as &quot;spread&quot;\nrequires to perform at least one transaction\n\ne.g.\n\narbitrageur can also do it with a flash loan\n\nhow to detect arbitrage/profitable opportunities?\n\nBellman Ford Algorithm\n\nNegative cycle detection\nWorks among multiple markets\nUsed in traditional finance and DeFi\n\nTheorem Solver (SMT tools aim to solve the satisfiability modulo theories ( #short SMT) problem)\n\nNeeds to encode the DeFi model\nApply heuristics for path pruning\n\nSandwich attacks\n\nP1 changes X to Y (value of X decreases and of Y increases)\nP2 changes also X to Y (value of X decreases even more and of Y increases more)\nP1 changes Y to X (gets more X out bc. Y is more valuable then before)\n\nprotection\n\noptimised trade execution\n\n+ simple\n- limited scope\n\ntrusted third party ordering\n\n+ efficient\n- no decentralisation\n\ncommittee ordering\n\n+ fairly efficient\n- reduced decentralisation\n\ncommit &amp; reveal\n\n+ secure\n- costly &amp; delay\n\nQuiz\n\nWhich of the following properties is NOT a desired property for AMM?\n\nno pool fees [correct bc. AMM pool should have some fees]\ninstant liquidity, irrespective of the trade size\npurchase of an asset decreases the asset supply in AMM, and increases its market price\nthe expected increase or decrease in price is based on the trading volume and available liquidity\n\nWhich of the following is a property for a typical on-chain order book?\n\nno fees for order placements\nfront running resistance\nimpermanent loss [there is no impermanent loss in an order book bc. market makers can fill the orders and thus liquidity cannot just be removed; this is rather in an AMM]\nnon-custodian ( #german Verwalter/Aufseher) settlement [correct bc. swapping of assets is non-custodial meaning you own your coins with your private key, there is no custodian doing the settlement]\n\nWhich of the following statements is incorrect\n\nUnexpected slippage can only cause a worse execution price [correct]\nunexpected slippage is caused by price change after a trade has been broadcast but before it has been confirmed\ntrades can configure a slippage protection threshold to limit the total slippage (expected + unexpected)\nThe concept of slippage is typically unavoidable in financial markets\n\nWhat is TURE about impermanent loss?\n\nimpermanent loss cannot result in the total loss of funds.\nTrading fees can never compensate the impermanent loss.\nImpermanent loss is impermanent bc. it's only realised upon a withdrawal from a liquidity pool. [correct]\nImpermanent loss is only a fictive accounting phenomenon and never materialises in a loss.\n\nBack-running describes the process where an adversary attempts to execute its own transaction immediately after the victim's transaction executes. For example, given two exchange markets A and B, both trading ETH/USD at a price of 1000. If a victim's trad (with a as price of 100GWei) is expected to push exchange A's price to 1100 ETH/USD, the adversay can attempt to back-run the victim to perform arbitrage between A and Ab. Which of the following method will have the highest back-running success rate?\n\nBroadcast the back-running transaction immediately after the victim's transaction with 99GWei\nBroadcast the back-running transaction immediately after the victim's transaction, with 100GWei\nSend a private back-running transaction to private relayer services, and bribe the miners to perform the back-run\n(i) broadcast the back-running transaction immediately after the victim with 100GWei, and also (ii) send the transaction to private relayer services and bribe the miners [correct]",
		"tags": ["short", "short", "short", "short", "german", "note"]
},

{
		"title": "DF L5 Lending & Stablecoins",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/df/df-l5-lending-and-stablecoins/",
		"content": "added multiple things in <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/df/df-definitions/\">DF Definitions</a>\n\nHow economic machine works\n#video How The Economic Machine Works by Ray Dalio\n\nTransaction: money, credit &lt;=&gt; goods, services, financial assets\n\ntransactions drive economy\nneeds buyers and sellers\n\nspecial actors\n\ncentral bank:\n\ncan print money\nsets interest rates\n\nstate: greatest buyer\n\ncredit\n\nmost important part bc. biggest and most volatile part\nneeds lenders and borrowers\n\nprincipal: amount lend\ninterest: additional amount to pay back for principal\n\ndebt is created with it\ncredit creates spending =&gt; drive economy\nwithout credit: growth can just come from more productivity\nwith credit: growth can come from more productivity and more spending through borrowing/credit =&gt; creates cycle\nbad when it supports overconsumption and cannot be paid bad e.g. big TV\ngood when it efficiently allocates resources e.g. tractor to harvest more crops\n\neconomic growth due to cycle of: (more) income =&gt; borrowing =&gt; spending =&gt; productivity =&gt; ...\ninflation when prices rise\n\ntoo much inflation causes problems =&gt; central bank rises interest rates =&gt; fewer people can borrow money =&gt; borrow less =&gt; less spending =&gt; prices go down =&gt; deflation =&gt; recession =&gt; decrease interest rates =&gt; more borrowing =&gt; inflation =&gt; ...\n\ndeflation when prices decrease\ndeleveraging\n\nworse than recession bc. lowering interest rates doesn't work (not rly. possible to go under 0%)\nbeautiful deleveraging through properly applying counter actions =&gt; debt declines same as income\n\ndeflationary (cut spending and dept + redistribution) and inflationary (printing money) actions are in balance\nincome needs to grow faster than debt grows =&gt; people are creditworthy again\ngrowth is slow but debt shrinks\n\npossible counter actions\n\ncut spending: has opposite effects bc. less spending =&gt; less income (for another person) =&gt; lower wages and unemployment =&gt; can pay back less\n\nand lenders notice that borrowers probably cannot pay back everything =&gt; don't lend anything anymore\n\ncut dept\n\nlenders don't want that and &quot;alternatives&quot; are: less paid back, over longer time frame, reduced interest rate\nalso deflationary bc. everything loses value and so borrowers still, also with less debt, won't have enough to pay back (even less)\n\nredistribution: government has to pay for unemployment and want's to set economic stimulus =&gt; needs more money generally tries to raise taxes for wealthy (bc. wealth is concentrated), but wealthy don't like that too much...\n\nrevolution/class conflict evolves =&gt; maybe army and/or (strong) political change e.g. Hitler after 1930s in Germany\n\nprint money: last alternative bc. interest rates are already lowered as much as possible\n\ninflationary and stimulates economy\nto buy financial assets and government bonds\n\njust helps those who own financial assets bc. central bank can only buy financial assets\n\ncentral bank can cooperate with central government through buying government bonds to spend money on goods and services =&gt; money for people\n\nvery risky\n\ndoesn't have to result in inflation, when credit shrinks the same amount and thus spending is not higher\n\nreflation (after deleveraging)\n\nca. 7-10 year (lost decade) (maybe just after a &quot;beatauiful&quot; deleveraging)\n\nLecture (graphics)\n\neffects one-by-one\neffects combined\n\nLending\nOn-Chain lending &amp; borrowing\n\nFlash loan\n\nflash loan should be taken and repaid in one single transaction\n\nnot directly time bound rather has to happen within one block building process\nif it cannot be paid back =&gt; entire transaction is invalid\n\nlender is secure\n\nUse cases\n\nDeFi attacks\n\nprice oracle manipulation\npump and dump (buy another coin to make them look like as more valuable/have more transactions for a short amount of time)\n\n(risk-free) arbitrage\nwashtrading\nflash mining\ncollateral swapping\nliquidation\n\nwhen liquidator doesn't have cryptocurrency upfront to repay\nonly works when liquidation completes in one transaction\ne.g.\n\nLiquidation\nin traditional finance\n\npass resolution for voluntary liquidation can be done by board of executives\nadministration of liquidation are e.g. the not paid electricity bills\ntakes a long time maybe even years\n\nFixed spread liquidation\n\ncan be completed in 1 transaction\nrepays debts of borrowing position\nacquires collateral at a discounted price from the position in return\n\ntypically discounts are e.g. 5-15% (called fixed spread) in Aave ( #disclaimerN probably meant average with it)\n\nprice oracles for getting prices: on and off chain oracles possible/depending on currency\n\nQuiz\n\nHow long does a flash loan last?\n\nDuring one block\nDuring one transaction [correct]\nFrom transaction signature until the transaction is mined\nas long as you want\n\nWhat is TRUE?\n\nLiquidations are typically done optimally.\nThe close factor is the minimum proportion of debt that can be repaid in a liquidation. [false bc. it's the maximum proportion... and you can also liquidate less]\nA flash loan has no fees\nTransaction fees for a flash loan are on the order of 100 USD, even if the loan amounts can grow beyond 1B USD. [correct]\n\nWhat can flash loans be used for?\n\nWashtrading [correct]\npumping a coin [false bc. you can just do pumping and dumping together, not individually]\ndumping a coin\nforking a blockchain\n\nWhich of the following liquidation strategies best describes an optimal fixed spread liquidation strategy?\n\nthe liquidation first performs an oracle price update, and then atomically performs the liquidation\nthe liquidator liquidates the position up to the close factor (when the close factor drops below 1, the position becomes available for liquidation)\nperform two liquidations. The first one keeps the close factor below 1, such that the second liquidation can also successfully execute [correct ]\nperform three liquidations, two keeping the CF below 1, while the last one also successfully executes\n\nWe define a borrowing position as a bad debt if it is financially rational for neither the borrowers nor the lending platform to close the position. Which of the following is not a cause for bad debt?\n\nif the collateral value falls below the value of the debt.\nif the value of excess asset used for over-liquidation cannot cover the liquidation transaction fee.\nif the debt has not been repaid for a long time. [correct bc. time component has no impact on bad dept]\nif the value of the debt grows more than the value of the collateral.",
		"tags": ["video", "disclaimerN", "note"]
},

{
		"title": "DF L6",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/df/df-l6/",
		"content": "Economic models\n\nrepresents (often simplifies) an economic process\ncontains <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/df/df-l6/#variables\">#Variables</a>\nlogical/quantitative relationships between variables\nexample: inflation\n\nmeasuring inflation requires a behaviour model bc. it depends on behaviour of individuals e.g. what/how much food they buy =&gt; how money is used\ndifferentiate relative vs. inflation price change\n\nVariables\n\nInterest rate(s) in CeFi and DeFi\ncryptocurrency price\ncollateral ratio\nDeFi protocol fees\nblockchain transaction fees\nnumber of users\nblockchain transaction throughput\n...\n\nExogenous vs. endogenous\n\nExogenous variable == outside force\n\ndetermined outside model, imposed on model\ne.g. in MakerDAO, asset price of collateral (e.g. ETH), is independent of the MakerDAO system\n\nEndogenous variable == inside the model\n\nStablecooins\nTypes\n\nTypes\n\nReserved-based e.g. USDC by a bank\ncollateral-based: e.g. MakerDAO\nalgorithmic e.g. AMPL\n\nMakerDAO\n\nto be profitable: assumption that ETH goes up and you can get even more with this DAI =&gt; with drawn DAI get even more ETH =&gt; later change back and have more in all\n\nAmpleforth\n#short AMPL\n\nsystemic implications #ToDo don't know yet\n\nUSDC and USDT\n\ndestroys coins\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/df/decentralised-finance/#bank-run\">Decentralised Finance#Bank run</a>\n\nQuiz\n\nWhich stable coin is the most stable? ( #remarkN atm)\n\nUSDC [correct]\nDAI\nAMPL\nETH\n\nWhich of the following statements is False?\n\nSeveral algorithmic stablecoins have depegged.\nDAI is a collateral based stablecoin, where e.g. ETH is used as collateral\nWBTC token prices should be pegged to BTC price\nETH is more stable than USDC [correct]\n\nWhat happens in AMPL system when 1 AMPL is worth less than 1 USD\n\nThe supply of AMPL is increase (expansion)\nThe supply of AMPL is decreased (contraction) [correct: if AMPL is more scarce, it should be worth more =&gt; less difference to USD]\nno action is taken (equilibrium)\nadditional collateral is required\n\nHow does the AMPL stablecoin reduce or increase the account balances of the token holders?\n\nThe ERC20 contract controls the balances and can modify them. [correct]\nThe ERC71 contract controls the balances and can modify them.\nevery user must individually approve each account balance change before rebalancing\nAMPL rebalances the token supply through arbitrage with USDC.\n\nIf the reserve ratio of a bank is 0.2 and the total money deposited by bank customers is $500,000, how much money is held by the bank?\n\n$100,000 [correct bc. 0.2 = 100'000/500'000 or 0.2 * 500'000 = 100'000]\n$250,000\n$400,000\n$1,000,000",
		"tags": ["Variables", "short", "ToDo", "remarkN", "note"]
},

{
		"title": "Decentralised Finance",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/df/decentralised-finance/",
		"content": "Promises\n\nInclusivity and Access:\n\nprovide financial services to individuals globally, including those who are unbanked or underbanked\npotentially bridge the gap between traditional financial systems and individuals who lack access to banking services\n\nPermissionless Innovation:\n\nplatforms are often open-source and permissionless, allowing developers to create and deploy financial applications without requiring approval from centralized authorities =&gt; rapid innovation and the creation of diverse financial products\n\nReduced Costs:\n\neliminating intermediaries and automating processes through smart contracts\npotential to reduce transaction costs associated with traditional financial services, such as remittances, loans, and trading\n\nDecentralized Identity and Privacy:\n\nleverage decentralized identity solutions, enhancing user privacy and reducing the reliance on centralized entities for identity verification\nUsers can control access to their personal information\n\nFinancial Empowerment:\n\nempower individuals by providing them with greater control over their financial assets\nUsers have custody of their private keys and can engage in financial activities without relying on traditional intermediaries\n\nTransparency: Blockchain technology, provides transparency by allowing users to trace transactions on a public ledger =&gt; enhance trust and accountability within the financial system\nInteroperability: platforms often operate on open and interoperable blockchain networks =&gt; allows different DeFi applications to seamlessly integrate with each other, creating a more interconnected and efficient financial ecosystem\nLiquidity Provision: platforms often use decentralized exchanges and liquidity pools, allowing users to provide liquidity and earn returns =&gt; creates new opportunities for individuals to participate in the financial ecosystem\n\nBank run\n\nin CeFi\n\ndangerous if fractional reserve\nmost clients don't receive assets\n\nin DeFi\n\nevent: USDT blacklists a pool, stablecoin de-pegs\ntraders will exit pools\nthose who exit first receive the best prices\ntry to build blacklist detection bots",
		"tags": [ "note"]
},

{
		"title": "Financial Markets",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/df/financial-markets/",
		"content": "Some things\nWhy?\nFinancial market serves several purposes, including:\n\nRaising capital: The financial market provides a platform for companies and governments to raise funds from investors in the form of equity or debt, which can be used to finance their operations or investment projects.\nPrice discovery: The financial market facilitates the determination of the prices of financial assets, based on the supply and demand of these assets. This price discovery process helps investors and market participants make informed investment decisions.\nLiquidity: The financial market provides liquidity to investors, allowing them to easily buy and sell financial assets such as stocks, bonds, and currencies. This enables investors to quickly and easily adjust their portfolios in response to changing market conditions.\nRisk management: The financial market allows investors to hedge against risks such as market volatility, inflation, and interest rate changes, by trading financial instruments such as options, futures, and swaps.\nEconomic growth: The financial market plays a crucial role in promoting economic growth by providing capital to companies and governments, which can be used to finance new projects and create jobs. This, in turn, can lead to increased economic activity and higher standards of living for individuals.\n\nMain stakeholders\n\nInvestors: Investors are individuals or entities who provide capital to companies and governments in exchange for financial assets, such as stocks, bonds, and other securities. Investors can be individual or institutional, such as pension funds, insurance companies, and mutual funds.\nIssuers: Issuers are companies or governments that offer financial assets to investors in order to raise capital. Issuers can issue debt in the form of bonds or equity in the form of stocks.\nIntermediaries: Intermediaries are entities that facilitate the trading of financial assets between investors and issuers. Examples of intermediaries include banks, broker-dealers, and stock exchanges.\nRegulators: Regulators are government agencies that oversee the financial market to ensure that it operates fairly and efficiently. Regulators set rules and guidelines for financial institutions and investors, and monitor the market to detect and prevent fraud and misconduct.\nMarket data providers: Market data providers are entities that collect and disseminate information about the financial market, including prices, trading volume, and other market data. This information is used by investors, issuers, and regulators to make informed decisions about the market.\n\nUnderlying assumptions from Economics\n\n(1) Rational Expectations: Based on perfect information\n\nEconomic actors will not make systematic mistakes in predicting the future (risks)\nEveryone uses the Â«rightÂ» model for forecasting\nThe future can be inferred from the past and present\n\n(2) Efficient Financial Market Theory: Asset prices represent the best possible estimates of the risks attached to them\n\nThe risk characteristics from financial markets can be inferred from mathematical analysis.\nMarket discipline can be used as an efficient tool in constraining harmful risk taking. Markets are self-correcting.\n\nWhy vulnerable?\nThe financial market is vulnerable to a variety of factors and risks, including:\n\nSystemic risks: These are risks that affect the entire financial system, rather than individual institutions or investors. Systemic risks can arise from a variety of sources, such as market volatility, geopolitical events, or regulatory changes.\nMarket risks: Market risks refer to the risk of losses due to changes in market conditions, such as changes in interest rates, exchange rates, or commodity prices.\nCredit risks: Credit risks refer to the risk of default by borrowers or issuers of debt securities. This can be due to factors such as financial distress, economic downturns, or changes in the creditworthiness of borrowers.\nOperational risks: Operational risks refer to the risk of losses due to internal failures of financial institutions, such as fraud, errors, or system failures.\nLiquidity risks: Liquidity risks refer to the risk that investors will not be able to buy or sell financial assets at fair market value due to a lack of market liquidity. This can lead to losses or a decrease in portfolio value.\nCybersecurity risks: With the increasing use of technology in the financial market, cybersecurity risks have become a significant concern. Cyber attacks on financial institutions can lead to the loss of sensitive data, financial theft, and disruption of market activity.\nOverall, the financial market is vulnerable to a wide range of risks, and investors and financial institutions must take steps to manage these risks in order to protect themselves and the broader financial system.",
		"tags": [ "note"]
},

{
		"title": "Unit type",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/atoms/unit-type/",
		"content": "#source #wikipedia Unit type\n\nIn the area of mathematical logic and computer science known as type theory, a unit type is a type that allows only one value (and thus can hold no information)\n\nProgramming languages\nHaskell\n\nThe () type can be thought of as a zero-element tuple. It's a type that can only have one value, and thus it's used where you need to have a type, but you don't actually need to convey any information. Here's a couple of uses for this.\n- #stackoverflow Neil Brown on StackOverflow\n\n#stackoverflow Longer explanation, also on types and expression language, besides ()",
		"tags": ["source", "wikipedia", "stackoverflow", "stackoverflow", "note"]
},

{
		"title": "FMFP Interpreters",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/fmfp-interpreters/",
		"content": "conceptually simple: read =&gt; evaluate =&gt; print\nconcretisation less trivial:\n\nMini-Haskell interpreter\nParsing\n\nparser constructs an abstract syntax tree ( #short AST)\n\nin Haskell: element of data type\nfurther processing depend on application but in general: conversion happens between data types e.g.\n\nCompiler: AST â†’ CODE\nCalculator: AST â†’ Int\nMini-Haskell: AST â†’ AST\n\nin ghci there are additional phases such as dependency analysis, type checking, ...\n\nCombinatory parsing\n\nIdea: modular construction and composition of parser functions\n\nuses lazy &amp; higher-order programming\nuses monads to package &quot;higher-order plumbing&quot;\n\nParser type\n\nparser is a function taking a string as input\nresult is an element of type a (typically a data type like Expr)\nparser may process only part of input, leaving a remainder\n\nsupport composition: pparses first part and q continues\n\nallow multiple results from parsing via list of successes\n\ndata Parser a = Prs (String -&gt; [(a,String)])\n\nparse::Parser a -&gt; String -&gt; String -&gt; [(a,String)]\nparse (Prs p) inp = p inp\n\n-- interested in result of first complete parse\ncompleteParse :: Parser a -&gt; String -&gt; a\ncompleteParse p inp\n\t| results == [] = error &quot;Parse unsuccessful&quot;\n\t| otherwise = head results\n\twhere results = [res | (res,&quot;&quot;) &lt;- parse p inp]\n\ncomplete parse if pair with remainder &quot;&quot;\n\nprimitive parsers\n\nserving as basic building blocks\n\n-- Fails trivially ([] signifies â€˜unsuccessful parseâ€™):\nfailure :: Parser a\nfailure = Prs (\\inp -&gt; [])\n\n-- Succeeds trivially without progress:\nreturn :: a -&gt; Parser a\nreturn x = Prs (\\inp -&gt; [(x,inp)])\n\n--Succeeds trivially with progress:\nitem :: Parser Char\nitem = Prs (\\inp -&gt; case inp of\n\t\t\t\t\t\t&quot;&quot; -&gt; []\n\t\t\t\t\t\t(x:xs) -&gt; [(x,xs)])\n\n-- Parse a single character with property p\nsat :: (Char -&gt; Bool) -&gt; Parser Char\nsat p = Prs (\\inp -&gt; case inp of\n\t\t\t\t\t\t&quot;&quot; -&gt; []\n\t\t\t\t\t\t(x:xs) -&gt; if p x then [(x,xs)] else [])\n\n--Chars and Strings (including simpler definition of sat)\nsat :: (Char -&gt; Bool) -&gt; Parser Char\nsat p = item &gt;&gt;= \\x -&gt; if p x then return x else failure\n\nchar :: Char -&gt; Parser Char\nchar x = sat (==x)\n\nstring :: String -&gt; Parser String\nstring &quot;&quot; = return &quot;&quot;\nstring (x:xs) = char x &gt;&gt; string xs &gt;&gt; return (x:xs)\n\n-- Repetition\nmany :: Parser a -&gt; Parser [a] -- 0 or more repetitions of p\nmany p = many1 p ||| return []\n\nmany1 :: Parser a -&gt; Parser [a] -- 1 or more repetitions of p\nmany1 p = p &gt;&gt;= \\t -&gt; many p &gt;&gt;= \\ts -&gt; return (t:ts)\n\n-- more readable use of &gt;&gt;-\nmany1 p = do t &lt;- p\n\t\t\t ts &lt;- many p\n\t\t\t return (t:ts)\n\nnumPos :: Parser Int\nnumPos = do ts &lt;- many1 (sat isDigit)\nreturn (read ts) --- read maps numeric string to number\n\nnumNeg :: Parser Int\nnumNeg = do char â€™-â€™\n\t\t\tt &lt;- numPos\n\t\t\treturn (-t)\n\t\t\t\nnum :: Parser Int\nnum = numPos ||| numNeg -- or: numPos +++ numNeg\n\nGluing parsers together\n-- Mutual selection: Apply both first and second parser\n(|||) :: Parser a -&gt; Parser a -&gt; Parser a\np ||| q = Prs (\\s -&gt; parse p s ++ parse q s)\n\n-- Alternative selection: If first parser fails, apply second parser\n(+++) :: Parser a -&gt; Parser a -&gt; Parser a\np +++ q = Prs (\\s -&gt; case parse p s of\n[] -&gt; parse q s\n\n-- Sequencing: first parser p then parser q to results\n(&gt;&gt;) :: Parser a -&gt; Parser b -&gt; Parser b\np &gt;&gt; q = Prs (\\s -&gt; [ (u,sâ€™â€™) | (t,sâ€™) &lt;- parse p s,\n\t\t\t\t\t\t\t\t(u,sâ€™â€™) &lt;- parse q sâ€™ ])\n-- but above results of first parser (t above) is lost bc. we just use the remainder further\n-- Solution: use as second argument a â€œparser generatorâ€ that takes as input the result of the first parser\n(&gt;&gt;=) :: Parser a -&gt; (a -&gt; Parser b) -&gt; Parser b\np &gt;&gt;= g = Prs (\\s -&gt; [ (u,sâ€™â€™) | (t,sâ€™ ) &lt;- parse p s,\n\t\t\t\t\t\t\t\t (u,sâ€™â€™) &lt;- parse (g t) sâ€™ ])\n\nambiguous grammars\n\nProblem: How should 2-3+4 be parsed?\nSolution\n\ndisambiguate grammar using associativity and precedence\ngive user ability to override defaults using parentheses\ncareful: left-recursive grammars lead to non-terminating recursion\n\nConcrete\n\nparse repeated operation/atom pairs after initial atom\nobtain left associativity using fold-left over list of these pairs\nuse concrete grammar to build abstract syntax tree of type data Expr = Lit Int | Add Expr Expr | Sub Expr Expr\n\nÎ»-calculus\n\nprograms are terms\nformalising core\n\nenough to construct all others\n\nParsing Î»-terms\n\ndata type for Î»-calculus terms\n\ndata Term = Id String | Ap Term Term | Lam String Term\n\t\t\tderiving Show\n\natom = ident ||| lamb ||| paren\n\nident = do id &lt;- identifier\n\t\t\treturn (Id id)\n\nterm= do t &lt;- atom -- t\n\t\tts &lt;- many atom -- [t1 t2 ... tn]\n\t\treturn (foldl Ap t ts)-- Ap(Ap(Ap(t t1) t2) ... tn)\n\nlamb= do token &quot;%&quot;\n\t\t ids &lt;- many1 identifier -- [x1, x2, ..., xn]\n\t\t token &quot;.&quot;\n\t\t t &lt;- term -- t\n\t\t return (foldr Lam t ids) -- Lam x1 (Lam x2 (...(Lam xn t)))\n\nparen = do token &quot;(&quot;\n\t\t t &lt;- term\n\t\t token &quot;)&quot;\n\t\t return t\n\nstr2term s = completeParse term s\n\napplication t1t2 produces left recursion (prefix-syntax simpler)\nsyntax without left-recursion\n\nWe use % and . instead of \\ and -&gt;, respectively\nExplicit parentheses\nEvery parsing starts with an identifier, or symbols â€˜%â€™ or â€˜(â€™\n\nExamples\n\nSubstitution in Haskell\n\nmust respect free and bound variables\n\ne.g.\n\nHaskell implementation below (Alternative: use Haskell library Data.Set to implement free)\n\nfree :: Term -&gt; [String]\nfree (Id v) = sing v -- free (x) = {x}\nfree (Ap s t) = union (free s) (free t) -- free (M N ) = free(M ) âˆª free(N )\nfree (Lam v t) = diff (free t) (sing v) -- free (Î»x. M ) = free(M ) \\ { x }\n\nempty = []\nsing a = [a]\n\nmember [] _ = False\nmember (x:xs) a\n\t| x &lt; a = member xs a\n\t| x == a = True\n\t| otherwise = False\n\nunion [] ys = ys\nunion xs [] = xs\nunion (x:xs) (y:ys)\n\t| x &lt; y = x : union xs (y:ys)\n\t| x == y = x : union xs ys\n\t| otherwise = y : union (x:xs) ys\n\ndiff [] _ = []\ndiff xs [] = xs\ndiff (x:xs) (y:ys)\n\t| x &lt; y = x : diff xs (y:ys)\n\t| x == y = diff xs ys\n\t| otherwise = diff (x:xs) ys\n\n-- subst t v s = t[v -&gt; s]\nsubst :: Term -&gt; String -&gt; Term -&gt; Term\nsubst (Id x) v s = if x == v then s else Id x\nsubst (Ap t1 t2) v s = Ap (subst t1 v s) (subst t2 v s)\nsubst (Lam x t) v s\n\t| x == v = Lam x t\n\t| not (member (free s) x) = Lam x (subst t v s)\n\t| otherwise = Lam z (subst (subst t x (Id z)) v s)\n\twhere z = fresh (union (free t) (free s))\n\t\tfresh m = (foldr max &quot;&quot; m) ++ &quot;â€™&quot; -- returns id not in m\n\nSubstitution\nÎ²-reduction\n\nÎ²-reduction is rule for simplifying redexes: (Î»x.M)Nâ†ªM[xâ†¦N]\n\nredex is a term like (Î»x.M)N\ncontractrum is the result i.e. M[xâ†¦N]\n\ne.g. (Î»x.f(xx))Nâ†ªf(NN)\n\nEvaluation strategies\n\nt1t2 represents the first application of a function to an argument\n\nfirst evaluate t1:t1â†ªr1\nIf r1â‰ Î»x.r then throw an exception (or return application)\n\nstrategy 1: Eager\n\nevaluate t2 prior to Î²-reduction: t2â†ªr2 meaning (Î»x.r)r2â†ªr[xâ†¦r2]\nevaluation carried out under an abstraction (Î»x.t)\n\nstrategy 2: Lazy\n\napply Î²-reduction to r1t2 i.e. substitute t2 without evaluation meaning (Î»x.r)t2â†ªr[xâ†¦t2]\nno evaluation under an abstraction\nresult of Î²-reduction is then further evaluated\n\nbeta (Lam x t) tâ€™ = subst t x tâ€™\neager :: Term -&gt; Term\neager (Id x) = (Id x)\neager (Ap t tâ€™) = case r of\n\t\t\t\t\t(Lam _ _) -&gt; eager (beta r râ€™)\n\t\t\t\t\t_ -&gt; Ap r râ€™\n\twhere r = eager t\n\t\t râ€™ = eager tâ€™\neager (Lam x t) = Lam x (eager t)\n\nlazy :: Term -&gt; Term\nlazy (Id x) = (Id x)\nlazy (Ap t tâ€™) = case r of\n\t\t\t\t\t(Lam _ _) -&gt; lazy (beta r tâ€™)\n\t\t\t\t\t_ -&gt; Ap r tâ€™\n\twhere r = lazy t\nlazy t = t -- no evaluation under a lambda abstraction\n\neager evaluation in lazy language\n\nHaskell is lazy and doesn't fully evaluate omega, since not needed to produce result\nsolution: use strict function application f $! x is like f x but forces evaluation of its argument x (up to first constructor)\n\neager :: Term -&gt; Term\neager (Id x) = (Id x)\neager (Ap t tâ€™) = case r of\n\t\t\t\t\t(Lam _ _) -&gt; eager ((beta $! r) $! râ€™)\n\t\t\t\t\t_ -&gt; (Ap $! r) $! râ€™\n\twhere r = eager t\n\t\t râ€™ = eager tâ€™\neager (Lam x t) = Lam x $! (eager t)\n\nMore on Haskell and interpreters",
		"tags": ["short", "note"]
},

{
		"title": "Haskell",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/haskell/",
		"content": "operators\n\nhave diff. binding strength e.g.\n\n^ binds stronger than + see ? 2 + 3^2 =&gt; 11\nnot stronger than &amp;&amp; and ||\n\norder and equality return True or False of type Bool\n\nsame as in other programming languages but\n\n/= unequal\n\nTypes\nInt\n\nInt type with at least the range {âˆ’229,â€¦,229âˆ’1}\n\nsupport for unbounded and arithmetic: Integer\n\nbool\n\nvalue: True, False\n\nChar\n\ne.g. 'a','0','\\t'\n? ord 'a' =&gt; 97 or ? chr 97 =&gt; 'a'\n\nString\n\ne.g. &quot;hello&quot;, &quot;123&quot;\n? &quot;Hello &quot; ++ &quot;there&quot; =&gt; &quot;Hello there&quot;\n\nDouble\n0.3456, -2.85e03\nTuple\n\nused to model composite objects (&quot;records&quot;)\nexample of a type constructor\n\nif T1,â€¦,Tn are types, then (T1,â€¦,Tn) is a (tuple) type\n\nList\n\nIf T is a type then [T] is a type\nempty list: []::[T]\nnon-empty list (x:xs)::[T] if x::T and xs::T\nabbreviations\n\n? [3..6] =&gt; [3,4,5,6]::Int\n`? [6..3] =&gt; []::Int\n[n, p..m] means count from n to m in steps of pâˆ’n\n\n? [7,6..3] =&gt; [7, 6, 5, 4, 3] :: [Int]\n? [0.0, 0.3 .. 1.0] =&gt; [0.0,0.3,0.6,0.8999999999999999] :: [Double]\n\nx:[y] appends an element x to a list [y]\n[x]++[y] concatenates two lists\n\nDifference lists\n\nfunction [a] -&gt; [a] that prepends a list to its argument\ne.g.\nimplementation\n\nOwn types\n\ntypes always start with a capital letter\ntype Person = String\ntype Database = [(Person,Book)]\n\nClass\n\ndefines a set of types\nelements of the class are called instances\n\nExamples\nallEqual :: Eq t =&gt; t -&gt; t -&gt; t -&gt; Bool -- where Eq is a class\n\nFunctions\n\nif argument doesn't matter use _ e.g. f a _ = a for f :: Int -&gt; Int -&gt; Int\nevaluation by\ngeneric type definition possible e.g. ownLength :: [a] -&gt; Int where a is the generic type\ndo when function has side-effects e.g. in IO\nown e.g.\n\nsumList::[Int] -&gt; Int\nsumList [] = 0 -- also handle the empty list case\nsumList (x:xs) = x + sumList xs\n\nstandard ones:\n\nlength\nappend\n[2] ++ [3,4,5] =&gt; [2,3,4,5]\n? 2 : [3,4,5] =&gt; [2,3,4,5] but ? [2] : [3,4,5] =&gt; Error\n\nHow to use\nInfix binary\nInfix binary function is also called an &quot;operator&quot; ? 7 'mod' 2\nprefix\nOperators can also be written in prefix notation: ? (+) 3 4\nExamples\n\n(e.g. for Int) +, * , ^, -, div, mod, abs\n\nevaluate by ? mod 7 2 =&gt; 1\n\nbranches\n\nif test then a else b\n\na and b have to be of same type\n\nmulti case\n\ndefined using other operators: f x y = (x || y) &amp;&amp; not (x &amp;&amp; y)\ndefined using guards\n\nf x y\n| x = not y\n| otherwise = y\n\ndefined using multiple cases (new)\n\npriority from above down\nexception, if one case is not covered but it's used\n\nf True True = False\nf True False = True\nf False True = True\nf False False = False\n\ncases can contain variables\n\nf True y = not y\nf False y = y\n\nf 0 = 1\nf 1 = 2\nf x = x*x\n\nAdvice on recursion\n\nDefining recursive functions is like riding a bicycle: it looks easy when someone else is doing it; it may seem impossible when you first try to\ndo it yourself, but becomes simple and natural with practice.â€\n- G. Hutton, Programming in Haskell\n\nHigher-order functions\n\njust higher order, if function takes function(s) as input\n\nÎ»-expression\n\nfor writing functions in-line\n\ngood for when functions are just used once and rather short\n\nInput-Output\n-- putStrin :: String -&gt; IO()\n\n-- getLine :: IO String\n\n-- read String :: t, where t is a type and it tries to convet String to this type\n\n-- show a =&gt; String of a\n\nfunc1 = do\nputStrLn &quot;Test&quot;\nn &lt;- getLine\nputStrLn (&quot;Text: &quot; ++ show (func2 (read n :: Int)))\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/monads/#input-ouput\">Monads#Input/Ouput</a>\n\nOther\nShell\n\nghci: open\n:load name-of-file.hs load modules from file\nreload loads last file again (e.g. after changing functions)\n\nLazy Evaluation\n\nHaskell uses Lazy evaluation\npossible problem of lazy evaluation is duplicated computation: avoided by simultaneously reducing both occurrences\nSummary: function arguments are evaluated only when needed and at most once\n\nCool applications\n\ncompute with infinite data in finite time\nimplementation of sieve of Eratosthenes\n\n2D layout\n\nspaces are important don't use TABs (in modern IDEs it should be okay, but this maybe a reason for an erroneous program)\nindentation determines separation of definitions\n\nall function definitions must start at same indentation level\nif a definition requires n &gt; 1 lines, indent lines 2 to n further\nrecommended layout\n\nPatterns\n\npurposes\n\nchecks if argument has proper form\nbinds values to variables\n\nRules\n\npatterns are inductively defined\npattern required to be linear i.e. each variable can occur at most once\nExamples: [(x,foo),_] , ((x,y),_) , 1:(2:(x,y))\nCounterexamples: (x ++ y, z) , [x,y,z,x]\n\nList comprehension\nNotation for sequential processing of list elements\n\nanalogous to set comprehension in set theory {2â‹…x|xâˆˆX}\nHaskell notation [2*x| x &lt;- xs]\ncan be augmented with guards: [2*x | x &lt;- xs, pred1(x), ...]\n\nExamples\n\n(x:xs) matches with [2,3,4] as x=2 and xs=[3,4]\n? let ([x,y,z],t) = ([1,2,3],(20,30)) in x + y =&gt; 3 :: Int\n? [2*x | x &lt;- [1,2,3,4,5]] =&gt; [2,4,6, 8, 10]\n? [n â€˜modâ€˜ 2 == 0 | n &lt;- [2,4,7]] =&gt; [True,True,False]\n? [2*x | x &lt;- [0,1,2,3,4,5,6], x â€˜modâ€˜ 2 == 0, x &gt; 3] =&gt; [8,12]\neasy quick sort^^\n\nq [] = []\nq (p:xs) = q [x | x&lt;-xs, x &lt;= p] ++ [p] ++ q [x | x&lt;-xs, x &gt; p]",
		"tags": [ "note"]
},

{
		"title": "FMFP L1",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l1/",
		"content": "Functional programming\n\nbasic concepts\n\nfunction (which are values itself) #aka first order citizens\nvalues\n\nexample programming languages: Haskell\n\nAdvantages\n\neasier to reason about bc. no state (changes) anymore\n\nno side effects\n\nreferential transparency every expression with same values evaluate to same expressions\nrecursion instead of iteration\nFlexible type system: many programming errors not possible (compile time error) e.g. 3 + TRUE, polymorphism supports reusability",
		"tags": ["aka", "note"]
},

{
		"title": "FMFP L10",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l10/",
		"content": "algebraic data types\n\ndata types defines a set of terms for each type instance e.g. Tree Int correspons to {Leaf, Node 0 Leaf Leaf, ...}\nalgebraic here means the smallest set S where LeafâˆˆS and xâˆˆaâˆ§t1âˆˆSâˆ§t2âˆˆSâŸ¹(NodeÂ xt1t2)âˆˆS\nIntuition: set S is built in steps\n\nLeafâˆˆS and\n(Nodext1t2)âˆˆS, where t1 and t2 in S in earlier steps\n\nStructural induction",
		"tags": [ "note"]
},

{
		"title": "FMFP L11",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l11/",
		"content": "<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/haskell/#lazy-evaluation\">Haskell#Lazy Evaluation</a> #remarkN maybe (probably, but don't know which one exactly^^) this was also done in a previous lecture\n\nArithmetic syntax tree #short AST\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/fmfp-interpreters/\">FMFP Interpreters</a>",
		"tags": ["remarkN", "short", "note"]
},

{
		"title": "FMFP L12",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l12/",
		"content": "<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/fmfp-interpreters/\">FMFP Interpreters</a>",
		"tags": [ "note"]
},

{
		"title": "FMFP L13",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l13/",
		"content": "<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/monads/\">Monads</a>",
		"tags": [ "note"]
},

{
		"title": "FMFP L14",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l14/",
		"content": "<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/monads/\">Monads</a>",
		"tags": [ "note"]
},

{
		"title": "FMFP L2",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l2/",
		"content": "<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/minigames/natural-deduction/\">Natural deduction</a>",
		"tags": [ "note"]
},

{
		"title": "FMFP L3",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l3/",
		"content": "Correctness\nWhat does it mean?\n\nTermination: important for many, but not all, programs\nFunctional behavior: function should return &quot;correct&quot; value. can be defined by another (mathematically defined) function or an input-output relation.\nCorrectness is rarely obvious =&gt; must be proven\n\nTermination\n\nif f is defined in terms of functions g1,â€¦,gk(giâ‰ f) and each gi terminates, then so does f.\n\nproblem is recursion, when some gi=f\n\nsufficient condition for termination: arguments are smaller along a well-founded order on functions domain\n\nan order &gt; on a set S is well-founded âŸº there is no infinite decreasing chain x1&gt;x2,â€¦ for xiâˆˆS\npossible to construct new well-founded relations from existing ones\n\nCorrectness\n\nexamples for proving correctness on slides: mainly using induction\n\nDifferent reasoning strategies\n\nequational reasoning bc. functions are equations\nreasoning by cases\n\nwith inference rules\n\nExcluded Middle (TND): For all propositions P holds Pâˆ¨Â¬P\nCase split (âˆ¨-E): Given Qâˆ¨R to prove any P we must prove\n\nP follows from Q and\nP follows from R\n\nproof by induction",
		"tags": [ "note"]
},

{
		"title": "FMFP L4",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l4/",
		"content": "<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/haskell/\">Haskell</a>",
		"tags": [ "note"]
},

{
		"title": "FMFP L7",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l7/",
		"content": "<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/mini-haskell/\">Mini-Haskell</a>",
		"tags": [ "note"]
},

{
		"title": "FMFP L8",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l8/",
		"content": "<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/haskell/\">Haskell</a>",
		"tags": [ "note"]
},

{
		"title": "FMFP L9",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/lectures/fmfp-l9/",
		"content": "Type classes\n\nmorphism\n\nmonomorphism: just one primitive type\npolymorphism: can take multiple primitive types\n\ntype classes restrict polymorphism\n\n-- from prelude.hs\nclass Eq a where\n\t(==) :: a -&gt; a -&gt; Bool\n\t(/=) :: a -&gt; a -&gt; Bool\n\n\tx /= y = not (x==y) -- thus in the end, a has just to be == comparable\n\t-- example with Eq\n\tallEqual :: Eq t =&gt; t -&gt; t -&gt; t -&gt; Bool\n\n-- instance example\ninstance Eq Bool where\n\tTrue == True = True\n\tFalse == False = True\n\t_ == _ = False\n\nelements of class are called instances\n\ninstance builds instances by &quot;interpreting signature functions&quot;\ninstances of primitive types like Int use built-in (primitive) qualities\n\nclass Visible t where\n\tstringOf :: t -&gt; String\n\tsize :: t -&gt; Int\n\t\ninstance Visible Char where\n\tstringOf ch = [ch]\n\tsize _ = 1\n\ninstance Visible Bool where\n\tstringOf True = &quot;Wahr&quot;\n\tstringOf False = &quot;Falsch&quot;\n\tsize b = 1\n\n? (stringOf â€™eâ€™) ++ &quot;ine &quot; ++ (stringOf True) ++ &quot;e Aussage&quot;\n&quot;eine Wahre Aussage&quot; :: [Char]\n\ninstance Visible t =&gt; Visible [t] where\n\tstringOf xs = concat (map stringOf xs)\n\tsize xs = foldr (+) 0 (map size xs)\n\t-- stringOf xs ... is not a recursive definition bc. on of them is over `[t]` and the other just over `t`\n\n? size [True,False]\n2 :: Int\n\n? stringOf [True,False]\n&quot;WahrFalsch&quot; :: [Char]\n\nif t is visible then a list of type [t] is also visible =&gt; class membership can depend on membership for other types\n\nDerived classes\nclass Eq a =&gt; Ord a where\n\t(&lt;), (&gt;), (&lt;=), (&gt;=) :: a -&gt; a -&gt; Bool\n\tmax, min :: a -&gt; a -&gt; a\n\t-- implement all operators except (&lt;=)\n\tx &lt; y = x &lt;= y &amp;&amp; x /= y\n\tx &gt;= y = y &lt;= x\n\tx &gt; y = y &lt;= x &amp;&amp; x /= y\n\nmax x y | x &lt;= y = y\n\t\t| otherwise = x\nmin x y | x &lt;= y = x\n\t\t| otherwise = y\n\nif a belong to Ord, then a must also belong to Eq\nfunctions for Eq are inherited and some new ones must be given\n\nSome type classes\n\nShow and Read\nFoldable\n\nEnumeration types (disjoint unions)\ndata Season = Spring | Summer | Fall | Winter\ndata Month = January | February | March | April | May | June | July | August |September | October | November | December\n\n-- functions can be written using pattern matching\nwhichSeason :: Month -&gt; Season\nwhichSeason January = Winter\n...\n\nSyntax\n\nstart with keyword data\nnames different (uniquely named) constructors\nfirst letter of each constructor must be upper-case\n\nDefines a set: Season = {Spring, Summer, Fall, Winter}\n\nProduct types\ndata People = Person Name Age\ntype Name = String\ntype Age = Int\n\n-- Constructors are functions\n? :type Person\nPerson :: Name -&gt; Age -&gt; Peopled\n\nshowPerson :: People -&gt; String\nshowPerson (Person n a) = n ++ &quot; who is &quot; ++ show a ++ &quot; years old&quot;\n\n? showPerson (Person &quot;Uncle George&quot; 85)\n&quot;Uncle George who is 85 years old&quot; :: [Char]\n\nan element of type People consists of a name n and an age a e.g. Person &quot;Uncle George&quot; 85\n\nProduct types versus tuples\nEnumeration and product types\n\ngeneral case\n\nIntegration with classes\n\nor in some cases, class instances can be automatically derived\n\nRecursive types\n\nrecursive functions over data types",
		"tags": [ "note"]
},

{
		"title": "Mini-Haskell",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/mini-haskell/",
		"content": "substantial simplification of Haskell, but the central core\n\nSyntax\nTyping\nProof system\nRules for core Î»-calculus\nFurther rules\nType inference\ntype inference failures\n\nSelf application",
		"tags": [ "note"]
},

{
		"title": "Evaluation",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/minigames/evaluation/",
		"content": "General\n\ninfix functions e.g. 1+2 = (+) 1 2 where + is infix\n\nand for (+a) b = b + a = (+)b a, a is the second argument and the following a\n\nfunctions are always applied to exactly one argument at a time\n\nLazy evaluation\nTo evaluate (t1t2)\n\nevaluate t1\nif the result of t1 is (Î»x.u), return u[x/t2]\nThen evaluate the result of the substitution\nTo evaluate (Î»x.t) return it unchanged\n\nEager evaluation\nTo evaluate (t1t2)\n\nevaluate t1, then evaluate t2 to t2â€²\nif the result of t1 is (Î»x.u), return u[x/t2â€²]\nThen evaluate the result of the substitution\nTo evaluate (Î»x.t), evaluate t to tâ€² and return (Î»x.tâ€²)\n\nCommon mistakes/special care\n\nincorrect parenthesis at the beginning: e.g. Î»x.x(Î»y.xy)â‰¢(Î»x.x)(Î»y.xy)\nsubstitution removes the lambda (Î»x.x)(Î»y.xy)â‡Ì¸Î»(Î»y.xy).Î»y.xy but just Î»y.xy\nin lazy evaluation there is no evaluation under lambdas\nin eager evaluation, need to evaluate argument before substitution",
		"tags": [ "note"]
},

{
		"title": "Natural deduction",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/minigames/natural-deduction/",
		"content": "Natural Deduction\n\n[!info]- Abstract example of a formal proof\n\nrules are used to construct derivations under assumptions\nproof is a derivation whose root has no assumptions (at the root on the left there is nothing)\nderivations are trees (normally done from bottom to top)\n\nPropositional logic\nSyntax\n\nIs a bird a formula? No, bc. it's about the smallest set (inductive definition), so there is not other stuff there!\n\nSemantics\n\nvaluation Ïƒ:Vâ†’{True,False} is a function mapping variables to truth values (truth assignment).\n\nvaluations are simple kinds of models (interpretations)\nLet Valuations be the set of valuations.\n\nSatisfiability:\nNote that ÏƒâŠ­âŠ¥ for every Ïƒâˆˆ Valuations: meaning falsity is never satisfied (as it should bebn )\na formula AâˆˆLP is satisfiable if ÏƒâŠ¨A, for some valuation Ïƒ\n\nvalid (a tautology) if it holds for all valuations Ïƒ\n\nsemantic entailment: A1,..AnâŠ¨A if for all Ïƒ, if ÏƒâŠ¨A1,â€¦,ÏƒâŠ¨An then ÏƒâŠ¨A\n\nrequirements for deductive system\n\nsyntactic entailment âŠ¢ (derivation rules) semantic entailment âŠ¨ (truth tables) should agree\nFor Î“â‰¡A1,â€¦,An some collection of formulae for which holds:\n\nsoundness: If Î“âŠ¢A can be derived, then Î“âŠ¨A\ncompleteness: If Î“âŠ¨A, then Î“âŠ¢A can be derived\n\n(desirable, but not necessary) Decidability: What is complexity of determining:\n\nIf a proposition A is satisfied by a valuation Ïƒ? =&gt; linear\nIf A is satisfiable? =&gt; exponential to check all possible evaluation and it's NP-complete\nIf A is a tautology? =&gt; it's in Co-NP\n\nNatural deduction for propositional formulae\nderivation rules\nFirst-Order Logic\nSyntax\n\nsignature consists of a set of function symbols F and a set of predicate symbols P (and their arities e.g. fk to indicate function/predicate symbol f has arity kâˆˆN)\n\nconstants are 0âˆ’ary function symbols\n\nLet V be a set of variables\nTerm, the terms of first-order logic, is the smallest set where\n\nxâˆˆTerm if xâˆˆV and\nfn(t1,â€¦,tn)âˆˆTerm if fnâˆˆF and tiâˆˆTerm for all 1â‰¤iâ‰¤n\n\nForm, the formulae of first-order logic, is the smallest set where\n\nâŠ¥âˆˆForm\npn(t1,â€¦,tn)âˆˆForm if pnâˆˆP and tjâˆˆTerm, for all 1â‰¤jâ‰¤n\nAâˆ˜BâˆˆForm if AâˆˆForm, BâˆˆForm, and âˆ˜âˆˆ{âˆ§,âˆ¨,â†’}\nQx.AâˆˆForm if AâˆˆForm,xâˆˆV and Qâˆˆ{âˆ€,âˆƒ}\n\nbound/free\n\nnames of bound variables are irrelevant, they just encode the binding structure\nÎ±-conversion: renaming bound variables is okay\n\nOperators: omitting paranthesis:\n\nBinary operators:\n\nâˆ§ stronger than âˆ¨ stronger than â†’\nassociates: â†’ to the right while âˆ§ and âˆ¨ do to the left\n\nnegation stronger than binary\nquantifiers extend to the right as far as possible i.e. end of line or )\n\nSemantics\n\nstructure is a pair S=âŸ¨US,ISâŸ© where US is a nonempty set, the universe, and IS is a mapping where\n\nIS(pn) is an n-ary relation on US, for pnâˆˆP\nIS(fn) is an n-ary (total) function on US, for fnâˆˆF\nAs shorthand, write pS for IS(p) and fS for IS(f)\n\nan interpretation is a pair I=âŸ¨S,vâŸ©, where S=âŸ¨US,ISâŸ© is a structure and v:Vâ†’US a valuation.\nThe value of a term t under the interpretation I=âŸ¨S,vâŸ© is written as I(t) and defined by\n\nI(x)=v(x), for xâˆˆV and\nI(f(t1,â€¦,tn))=fS(I(t1),â€¦,I(tn))\n\nSatisfiability\nSubstitution\n\nreplace in a formula all occurences of a free variable x with some term t e.g.\nsubstitution must avoid capture of variables: if they would have the same name, then we have to first rename bound variables with Î±-conversion\n\nSymbols\nEquality\n\nlogical symbol wi",
		"tags": [ "note"]
},

{
		"title": "Type inference",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/minigames/type-inference/",
		"content": "Formal type inference\nStep-by-step\n\nStart with judgement âŠ¢t::Ï„0 where Ï„0 is a fresh type variable\nBuild derivation tree bottom-up by applying rules\nintroduce fresh type variables Ï„i and collect constraints if needed\nSolve constraints using unification to get possible types\n\nExample for resolving constraints\nExample unification\n\nInformal type inference\n\n(see step-by-step from formal type inference but) skip tree building and directly start unifying\n\nCommon mistakes/car\n\ncare for infix functions e.g. 1+2=(+)12\nfunctions are always applied to exactly one argument at a time\nbe careful when parsing x(&lt;) not same as (x&lt;)\nNumeric constants are polymorphic in Haskell e.g.\n\nExamples\n\nExercises with explanations by TA Ramon Wick",
		"tags": [ "note"]
},

{
		"title": "Monads",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/fmfp/monads/",
		"content": "#Idea separate values from computation producing the values\n\nf :: a -&gt; b ordinary function, returns value of type b\nf :: a -&gt; M b monadic function, returns computation M b\n\nM is a type constructor satisfying certain properties (monad laws)\nby varying M, we can model different notions of computation\n\nMonad is a type constructor in the monad type class\nimagine a monad as a container which &quot;wraps&quot; a value\nexplains side-effects in a functional context and helps designing controlled side-effects\n\nfine-grained control of side effects possible\ncan model computational effects that are not present in imperative languages\n\nProperties\n\nevery monad supports 2 basic operations\n\nembedding a value into a computation\ncomposing computations\n\nclass Monad m where\n\t-- return and bind are mathematical core\n\treturn :: a -&gt; m a\n\t(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b -- bind\n\t\n\t-- shortcut for convenience, when second computatin\n\t-- doesn't depend on result of first\n\t(&gt;&gt;) :: m a -&gt; m b -&gt; m b\n\tm1 &gt;&gt; m2 = m1 &gt;&gt;= (\\_ -&gt; m2)\n\t\n\t-- not partof mathematical concept of a monad\n\t-- called on pattern matching error in do-notation\n\tfail :: String -&gt; m a\n\n-- instances\ninstance Monad Maybe where\n\treturn x = Just x\n\tNothing &gt;&gt;= _ = Nothing\n\t(Just x) &gt;&gt;= f = f x\n\nMonad laws\nMonad operations must satisfy\n\n(1) return x &gt;&gt;= f = f x left unit\n(2) m &gt;&gt;= return = m right unit\n(3) (m &gt;&gt;= f) &gt;&gt;= g = m &gt;&gt;= (\\x -&gt; (f x &gt;&gt;= g)) associativity\n\nExamples\nmonad\ndata Maybe a = Nothing | Just a\n\n-- e.g. for making a safe division\nsafeDiv::Int -&gt; Int -&gt; Maybe Int\nsafeDiv n d\n\t| d /= 0 = Just (n `div`d)\n\t| otherwise = Nothing\n\nComputation with monads\n-- idea: computation takes a state of type s and transforms it into\n-- a result of type a and a successor state of type s\ndata State s a = State (s -&gt; (a,s)) -- here s is just a type, not an argument so in reality s on left hand side doesn't have to be s on right hand side, but they just have to be the same type!\n\n-- state access: read current value of state without changing it\nget :: State s s\nget = State (\\s -&gt; (s,s))\n\n-- state update: write a new state value ignoring the current state\nput :: s -&gt; State s ()\nput t = State (\\s -&gt; ((), t))\n\n-- auxiliary function that opens monad and\n-- runs computation from initial state s0\nrunState :: (State s a) -&gt; (s -&gt; (a, s))\nrunState (State m) s0 = m s0 -- bc. `(State m)` returns a function (see data State s a) which takes s0 as an argument; I just don't know yet, why this is not reflected in the data type of runState (why there is not another input)\n\n-- return embeds a value into a stateful computation\nreturn :: a -&gt; State s a\nreturn x = State (\\s -&gt; (x,s))\n\n-- Bind composes two stateful computations with value binding\n(&gt;&gt;=) :: State s a -&gt; (a -&gt; State s b) -&gt; State s b\nm &gt;&gt;= k = State (\\s -&gt; let (x, t) = runState m s\n\t\t\t\t\t\tin runState (k x) t)\n\napplication of state monad in counter\n\nstate monad encapsulate program composition\nrun program: invoke runState tick s0 where s0 is some initial state\n\napplication of state monad in tree renaming\n\nas reference: without state monad =&gt; ugly plumbing needed to thread state through two recursive calls\nstate monad takes care of all the plumbing\nor with more abstraction\n\nInput/Ouput\n\nproblematic bc. we don't know what we get as an input from user =&gt; reasoning would no longer be sound\n\nbc. result depends on order in which the argument are evaluated\nI/O-functions have side effects =&gt; state of world ( #disclaimerN nice name^^) changes\ne.g. IO Int not same as Int e.g. you cannot subtract bc. IO Int is monadic\n\ninputInt :: IO Int\ninputString :: IO String\noutputInt :: Int -&gt; IO ()\n\n() is the unit type in Haskell <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/fmfp/atoms/unit-type/#haskell\">Unit type#Haskell</a>\n\nBasic Actions\n\nSequencing\n\nExamples\n-- printing string on screen\nputString :: String -&gt; IO ()\nputString &quot;&quot; = return ()\nputString (x:xs) = do putChar x; putString xs\n\n-- reading string from keyboard\ngetString :: IO String\ngetString = do c &lt;- getChar\n\t\t\t\tif c == â€™\\nâ€™\n\t\t\t\tthen return &quot;&quot;\n\t\t\t\telse do cs &lt;- getString\n\t\t\t\t\treturn (c:cs)\n\t\t\t\t\t\n-- hello world (what else)\nmain :: IO ()\nmain = do putString &quot;Hi, I am HAL. Who are you?\\n&quot;\nname &lt;- getString\nputString (&quot;Hello &quot; ++ name ++ &quot;!\\n&quot;)\n\nFunctors\n\nwhen you instantiate Monad, you must instantiate Functor and Applicative too\n\nclass Functor f where\nfmap :: (a -&gt; b) -&gt; f a -&gt; f b\n\nLaws\n\nFunctor instances must satisfy two functor laws\n\nIdentity: fmap id v = v\nComposition: fmap f (fmap g v) = fmap (f . g) v\n\noperations\n\nin applicative functor mimicks function application #ToDo find symbols for LaTex\n\nclass Functor f =&gt; Applicative f where\npure :: a -&gt; f a\n(&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b -- associates to the left\n\ninstance Applicative Maybe where\npure x = Just x\nJust f &lt;*&gt; Just x = Just (f x)\n_ &lt;*&gt; _ = Nothing\n\nApplicative functor laws\n\nIdentity pure id &lt;*&gt; v = v\nHomomorphism pure f &lt;*&gt; pure x = pure (f x)\nComposition pure (.) &lt;*&gt; u &lt;*&gt; v &lt;*&gt; w = u &lt;*&gt; (v &lt;*&gt; w)\nInterchange v &lt;*&gt; pure x = pure (\\f -&gt; f x) &lt;*&gt; v\nfmap fmap f v = pure f &lt;*&gt; v\n\ncanonical implementation for monads\n\npure x = return x\nu &lt;*&gt; v = u &gt;&gt;= \\f -&gt; v &gt;&gt;= \\x -&gt; return (f x)\n\napplicative functor laws can be derived from monad laws\n\nExamples\n\nmap function transforms all elements in a structure: type class Functor can capture this functionality\n\ninstance Functor Tree where\nfmap _ Leaf = Leaf\nfmap f (Node x l r) = Node (f x) (fmap f l) (fmap f r)\n\n-- in console\n&gt; fmap (*2) [0,1,2] = [0,2,4]\n&gt; fmap (*2) (Node 5 Leaf Leaf) = Node 10 Leaf Leaf\n\napplicative functor examples\n\nResources\n\nrecommended monad tutorials",
		"tags": ["Idea", "disclaimerN", "ToDo", "note"]
},

{
		"title": "Appendix",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/appendix/",
		"content": "Kombinatorik\n\nPermutationen: n!=nâˆ—(nâˆ’1)âˆ—â‹¯âˆ—2âˆ—1\n\nFrage: Auf wie viele Arten kann man n Objekte anordnen?\nHerleitung: erste Objekt aus n Objekten wÃ¤hlen, zweite aus nâˆ’1 usw.\n\nKombinationen (nk)=n!k!(nâˆ’k)!\n\nFrage: Auf wie viele Arten kann man kâ‰¤n aus den n Objekten ohne ZurÃ¼cklegen auswÃ¤hlen.\nHerleitung:\n\nerste Objekt aus n, zweite aus nâˆ’1 wÃ¤hlen usw. d.h. nâˆ—(nâˆ’1)âˆ—â‹¯âˆ—(nâˆ’k+1)=n!(nâˆ’k)! Sequenzen der LÃ¤nge k\naber wir interessieren uns nicht fÃ¼r die Reihenfolge und teilen dadurch durch die mÃ¶glichen Permuationen d.h. durch k! (siehe Permutationen)\n\nVariationen: nm\n\nFrage: Wie viele Sequenzen der LÃ¤nge m kann man mit den n Elementen bilden?\nHerleitung: FÃ¼r jeden der m PlÃ¤tze sind jeweils n Elemente zur Auswahl.\n\nExample",
		"tags": [ "note"]
},

{
		"title": "Diskrete Wahrscheinlichkeit",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/diskrete-wahrscheinlichkeit/",
		"content": "Oft ist im Zusammenhang mit Andwendungen aus der Informatik Î©={Ï‰1,â€¦,Ï‰N} endlich oder Î©={Ï‰1,Ï‰2,â€¦}abzÃ¤hlbar. Dann...\n\nwird F=P(Î©), d.h. jede Teilmenge von Î© ist ein (beobachtbares Ereignis), gewÃ¤hlt.\nist P definiert durch die Wahrscheinlichkeiten pn:=P[Ï‰n], n=1,â€¦,N bzw. nâˆˆN, aller Elementarereignisse. Denn fÃ¼r jede beliebe Menge AâŠ†Î© gilt AâˆˆF undP[A]=P[â‹ƒÏ‰nâˆˆA{Ï‰n}]=âˆ‘Ï‰nâˆˆAP[{Ï‰n}]=âˆ‘{n|Ï‰nâˆˆA}pn\n\n[!info] #Def 1.14 Laplace Modell\nSei Î©={Ï‰1,â€¦,Ï‰n} mit |Î©|=N ein endlicher Grundraum. (Î©,F,P) heisst Laplace Modell auf Î©, wenn\n\nF=P(Î©)\nP ist die diskrete Gleichverteilung auf Î©, d.h. alle Elementarereignisse sind gleich wahrscheinlich,p1=â‹¯=pN=1N. Insbesondere gilt fÃ¼r belibiege AâŠ†Î©, P[A]=|A||Î©|",
		"tags": ["Def", "note"]
},

{
		"title": "WS L1",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/lectures/ws-l1/",
		"content": "Definitions\n\n[!info] #Def 1.2 Grundraum ( #aka Ereignisraum) ( #english sample space)\nÎ©â‰ âˆ… ist die Menge alle mÃ¶glichen Ergebnisse des betrachteten Zufallsexperiments.\n\nDie Elemente wâˆˆÎ© heissen Elementarereigniss #aka AusgÃ¤nge des Experiments #english outcomes\nWir sagen, dass ein Ereignis A eintritt, falls das realisierte Elementarereignis Ï‰ in A liegt d.h. Ï‰âˆˆA\n\n[!info] #Def 1.4 Die Potenzmenge ( #english power set)\nvon Î©, P(Î©) oder 2Î©, ist die Menge aller Teilmengen von Î©.\n\nEin prinzipielles Ereignis ( #english event) ist eine Teilmenge AâŠ†Î©, also eine Kollektion von Elementarereignissen\nDie Klasse aller (beobachtbaren) Ereignisse bezeichnen wir mit F. Das ist eine Teilmenge der Potenzmenge von Î©\n\nFalls Î© endlich oder abzÃ¤hlbar, dann ist oft F=P(Î©), und das ist ein diskreter Wahrscheinlichkeitsraum.\nFalls Î© Ã¼berabzÃ¤hlbar, muss F eine echte Teilklasse von P(Î©) sein\nIn jedem Fall muss F gewisse Axiome erfÃ¼llen\n\n[!Info] #Def 1.5 Ïƒ-Algebra (manchmal Ïƒ-field)\nEin Mengensystem FâŠ†P(Î©) nennt man eine Ïƒ-Algebra, wenn\n\nÎ©âˆˆF\nfÃ¼r jedes AâˆˆF auch das Komplement AâˆâˆˆF ist\nfÃ¼r jede Folge (An)nâˆˆN mit AnâˆˆF,nâˆˆN, auch die Vereinigung âˆªnâˆˆNAnâˆˆF ist\n\n[!info] #Def 1.9 Wahrscheinlichkeitsmass ( #english probability measure)\nSei Î© ein Grundraum und sei F eine Ïƒ-Algebra. Eine Abbildung\nP:Fâ†’[0,1],mitÂ Aâ†¦P[A]heisst Wahrscheinlichkeitsmass auf (Î©,F), wenn die folgenden Axiome erfÃ¼llt sind\n\nNormiertheit: P[Î©]=1\nÏƒ-AdditivitÃ¤t: P[âˆªnâˆˆNAn]=âˆ‘n=1âˆP[An] fÃ¼r paarweis disjunkte Mengen An, d.h. Anâˆ©Am=âˆ… fÃ¼r alle nâ‰ m\n\n[!info] #Proposition 1.10\nFÃ¼r ein Wahrscheinlichkeitsmass P auf (Î©,F) und Mengen A,BâˆˆF gelten folgende Aussagen:\n\nP[Aâˆ]=1âˆ’P[A], und insbesondere P[âˆ…]=0\nMonotonie: wenn AâŠ†B, dann P[A]â‰¤P[B]\nAdditionsregel: P[A]+P[B]=P[AâˆªB]+P[Aâˆ©B]\n\n[!info] #Def 1.12 Wahrscheinlichkeitsraum\nSei Î© ein Grundraum, F eine Ïƒ-Algebra und P ein Wahrscheinlichkeitsmass auf (Î©,F). Das Tripel (Î©,F,P) heisst Wahrscheinlichkeitsmass ( #english probability space)\n\n[!info] #disclaimer 1.13 Messbarer Raum ( #english measurable space)\nAllgemein verwenden wir in der Masstheorie folgende Terminologie:\n\nFÃ¼r eine Ïƒ-Algebra A auf einer Grundmenge Î© wird das Paar (Î©,A) ein messbarer Raum genannt.\nElemente AâˆˆA, also Teilmengen AâŠ‚Î©, die wir messen wollen, heissen messbare Mengen ( #english measurable sets).\nAuf messbaren RÃ¤umen (bzw. den Ïƒ-Algebren) lassen sich Masse ( #english measures) Î¼ definieren. Diese sind im Allgemeinen nicht auf 1 nomiert. Man verlangt staddessen Î¼(âˆ…)=0.\nDas Tripel $(\\Omega,\\cal{A},\\mu) $ heisst Massraum ( #english measure space).\nIst das Mass normiert, Î¼(Î©)=1, dann ist das Mass ein Wahrscheinlichkeitsmass und der Messraum wird zum Wahrscheinlichkeitsraum.\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/ws/diskrete-wahrscheinlichkeit/\">Diskrete Wahrscheinlichkeit</a>",
		"tags": ["Def", "aka", "english", "aka", "english", "Def", "english", "english", "Def", "Def", "english", "Proposition", "Def", "english", "disclaimer", "english", "english", "english", "english", "note"]
},

{
		"title": "WS L2",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/lectures/ws-l2/",
		"content": "[!info] #Def 1.22 Bedingte Wahrscheinlichkeit\nGegeben: Wahrscheinlichkeitsraum (Î©,F,P), P[B]&gt;0\nDann ist P[A|B]:=P[Aâˆ©B]P[B]\n\nP[A|A]=1\nP[A|Î©]=P[A]\nP[Aâˆ©B]=P[A|B]P[B]\nP[A|B] ist nicht definiert fÃ¼r P[B]=0\n\n[!info] #Theorem 1.25\nGegeben: Wahrscheinlichkeitsraum (Î©,F,P),P[B]&gt;0\nDann ist Pâˆ—:Fâ†’[0,1] definiert durch Aâ†¦Pâˆ—[A]:=P[A|B] wieder ein Wahrscheinlichkeitsmass auf (Î©,F)\n\nbedingte Wahrscheinlichkeit ist nicht symmetrisch in den beiden Argumenten. Insbesondere ist bei fixiertem Ereignis A die Funktion Bâ†¦P[A|B] kein Wahrscheinlichkeitsmass\nIst Î© endlich oder abzÃ¤hlbar mit F=P(Î©), dann ist P gegeben durch die Wahrscheinlichkeiten pn=P[{Ï‰n}]. Das bedingte Wahrscheinlichkeitsmass Pâˆ—[â‹…]=P[â‹…|B] ist dann durchÃ¼Ã¼pnâˆ—=Pâˆ—[{Ï‰n}]=P[{Ï‰n}|B]={pnP[B]fÃ¼rÂ Ï‰nâˆˆB,0fÃ¼rÂ Ï‰nâ‰ B,gegeben. Wir setzen alle Gewichte ausserhalb von B auf Null und skalieren die Gewichte in B mit einem festen Faktor, sodass ihre Summe wieder 1 ergibt.\n\n[!info] #Theorem 1.29 Satz von der totalen Wahrscheinlichkeit\nGegeben: B1,â€¦,BN mit P[Bn]&gt;0 fÃ¼r jedes 1â‰¤nâ‰¤N eine Partitione des Grundraums Î©, d.h. â‹ƒn=1NBn=Î© mit Bnâˆ©Bm=âˆ… fÃ¼r nâ‰ m.\nDann gilt fÃ¼r alle AâˆˆF\nP[A]=âˆ‘n=1NP[A|Bn]P[Bn]\n\n[!info] #Theorem 1.32 Satz von Bayes\nGegeben: B1,â€¦,BNâˆˆF eine Partition von Î© mit P[Bn]&gt;0 fÃ¼r alle n.\nFÃ¼r jedes Ereignis A mit P[A]&gt;0 und jedes nâˆˆ{1,â€¦,N} gilt\nP[Bn|A]=P[A|Bn]P[Bn]âˆ‘k=1NP[A|Bk]P[Bk]\n\nSpezialfall n=2 mit Î©=BâˆªBâˆ so ist\nP[B|A]=P[A|B]P[B]P[A|B]P[B]+P[A|Bâˆ]P[Bâˆ]\n\n[!info] UnabhÃ¤ngigkeit zweier Ereignisse\n(WS #Def 1.35)\nGegeben: (Î©,F,P) ein Wahrscheinlichkeitsraum\nZwei Ereignisse A und B heissen (stochastisch) unabhÃ¤ngig, falls\nP[Aâˆ©B]=P[A]P[B]\n\nFalls P[A]âˆˆ{0,1}, dann ist A unabhÃ¤ngig von jedem Ereignis\nFalls Ereignis von sich selbst unabhÃ¤ngig ist, dann muss P[A]âˆˆ{0,1} gelten. ( #disclaimerN (falls richtig) A von sich selbst unabhÃ¤ngig âŸºP[A]âˆˆ{0,1})\nA ist unabhÃ¤ngig von BâŸºA unabhÃ¤ngig von Bâˆ\n\n[!info] (WS #Proposition 1.37)\nGegeben: A,BâˆˆF zwei Ereignisse mit P[A],P[B]&gt;0\nDann sind folgende Aussagen Ã¤quivalent:\n\n(i) P[Aâˆ©B]=P[A]P[B] d.h. A und B sind unabhÃ¤ngig\n(ii) P[A|B]=P[A] d.h. Eintreten von B hat keinen Einfluss auf A\n(iii) P[B|A]=P[B] d.h. Eintreten von A hat keinen Einfluss auf B\n\n[!info] UnabhÃ¤ngigkeit (WS #Def 1.40)\nGegeben: I eine beliebige Indexmenge\nEine Familie von Ereignissen ()Ai)iâˆˆI heisst (stochastisch) unabhÃ¤ngig, wenn fÃ¼r alle endlichen Teilmengen JâŠ‚I gilt: P[â‹‚jâˆˆJAj]=âˆjâˆˆJP[Aj]\n\nfalls Menge unabhÃ¤ngig âŸ¹ Ereignisse paarweise unabhÃ¤ngig (Umkehrung gilt nicht!)\n\n[!info] Zufallsvariable (WS #Def 2.1)\nGegeben: Wahrscheinlichkeitsraum (Î©,F,P)\nEine (reellwertige) Zufallsvariable (Z.V.) ist eine Abbildung X:Î©â†’R, sodass fÃ¼r alle xâˆˆR gilt,\n{Ï‰âˆˆÎ©|X(Ï‰)â‰¤x}âˆˆF",
		"tags": ["Def", "Theorem", "Theorem", "Theorem", "Def", "disclaimerN", "Proposition", "Def", "Def", "note"]
},

{
		"title": "WS L3",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/lectures/ws-l3/",
		"content": "[!info] #Def 2.1 Zufallsvariable\nGegeben: Wahrscheinlichkeitsraum (Î©,F,P)\nEine (reelwertige) Zufallsvariable ( #short Z.V.) ist eine Abbildung X:Î©â†’R sodass fÃ¼r alle xâˆˆR gilt\n{Ï‰âˆˆÎ©|X(Ï‰)â‰¤x}âˆˆF\n\n[!info] #Remark 2.5 Messbarkeit\nWenn eine (reelwertige) Funktion die Eigenschaft (2.1) erfÃ¼llt, heisst diese Funktion messbar ( #english measurable).\nDa wir an AusgÃ¤ngen von Zufallsexperimenten interessiert sind, wollen wir Wahrscheinlichkeiten der Form\nÃ¼P[{Ï‰âˆˆÎ©|X(Ï‰)âˆˆB}]Â fÃ¼r bestimme MengenÂ BâŠ‚Rberechnen kÃ¶nnen. Z.B.\n\nFalls B={2,4,6} wenn wir die Wahrscheinlichkeit fÃ¼r einen gerade Zahl beim WÃ¼rfeln mÃ¶chten.\nWeil das Wahrscheinlichkeitsmass P auf F definiert ist, mÃ¼ssen die Urbilder aller dieser Mengen B,\n\nXâˆ’1(B):={Ï‰âˆˆÎ©|X(Ï‰)âˆˆB}in F enthalten sein.\n\nIn dieser Vorlesung verlangen wir fÃ¼r Messbarkeit, dass Xâˆ’1(B)âˆˆF fÃ¼r alle BâˆˆB(R)\n\nHierbei ist B(R) die borelsche Ïƒ-Algebra auf R\nz.B. alle offenen, abgeschlossenen und kompakten Mengen in R oder alle Intervalle der Form (a,b),[a,b],(a,b],[a,b),(âˆ’âˆ,b),(âˆ’âˆ,b],(a,âˆ),[a,âˆ) fÃ¼r a,bâˆˆR\n\n[!info] #Def 2.10 Verteilungsfunktion\nGegeben: reelwertige Zufallsvariable X, Wahrscheinlichkeitsraum (Î©,F,P)\nDie (kumulative) Verteilungsfunktion von X ( #english cumulative distribution function) ( #short cdf) ist die Funktion FX:Râ†’[0,1], definiert durch\nFX(x):=P[Xâ‰¤x]\n\n[!info] #Theorem 2.13 Eigenschaften von Verteilungsfunktionen\nGegeben: Zufallsvariable X, Wahrscheinlichkeitsraum (Î©,F,P)\nDie Verteilungsfunktion FX:Râ†’[0,1] von X erfÃ¼llt folgende Eigenschaften:\n\n(i) FX ist monoton wachsend\n(ii) FX ist rechtsstetig d.h. fÃ¼r alle xâˆˆR gilt FX(x)=limhâ†’0FX(x+h)\n(iii) Es gelten die Grenzwerte limxâ†’âˆ’âˆFX(x)=0 und limxâ†’âˆFX(x)=1\nI also found\nwikipedia\n\nP(Xâ‰¤x)=FX(x)\nP(a&lt;Xâ‰¤b)=FX(b)âˆ’FX(a)\nP(X=b)=FX(b)âˆ’limxâ†’bâˆ’FX(x)\n\nMath Stackexchange\n\nP(a&lt;X&lt;b)=P(X&lt;b)âˆ’P(Xâ‰¤a)=limxâ†’bâˆ’FX(x)âˆ’FX(a)\n\n[!info] #Def 2.16 Gemeinsame Verteilungsfunktion\nGegeben: Zufallsvariablen X1,â€¦,Xn\nDie gemeinsame Verteilungsfunktion von X1,â€¦,Xn ist die Abbildung F:Rnâ†’[0,1] definiert durch\n(x1,â€¦,xn)â†¦F(x1,â€¦,xn)=P[X1â‰¤x1,â€¦,Xnâ‰¤xn]\n\n[!info] #Def 2.18 <a class=\"internal-link is-unresolved\" href=\"/404\" target=\"\">UnabhÃ¤ngigkeit</a> von Zufallsvariablen\nGegeben: Zufallsvariablen X1,â€¦,Xn auf Wahrscheinlicheitsraum (Î©,F,P). Dann heissen X1,â€¦Xn unabhÃ¤ngig, wenn fÃ¼r alle x1,â€¦,xnâˆˆR gilt\nP[X1â‰¤x1,â€¦,Xnâ‰¤xn]=P[X1â‰¤x1]âˆ—â‹¯âˆ—P[Xnâ‰¤xn]\n#Remark 2.19 X1,â€¦,Xn sind genau dann unabhÃ¤ngig, wenn fÃ¼r alle Intervalle IâŠ‚R,â€¦,InâŠ‚R die Ereignisse {X1âˆˆI1},â€¦{XnâˆˆIn} unabhÃ¤ngig sind.\nWenn wir eine Menge unabhÃ¤ngiger Zufallsvariablen haben und disjunkte Gruppen solcher Zufallsvariablen bilden, dann sind diese Gruppen auch wiederum unabhÃ¤ngig voneinander.\n\n[!info] #Theorem 2.21 Gruppierung von Zufallsvariablen\nGegeben: unabhÃ¤ngige Zufallsvariablen X1,â€¦Xn, Indexe 1â‰¤i1&lt;i2&lt;â‹¯&lt;ikâ‰¤n, Abbildungen Ï†1,â€¦,Ï†k\nDann sind\nY1:=Ï†1(X1,â€¦,Xi1),Y2:=Ï†2(X1,â€¦,Xi2),â€¦,Yk:=Ï†k(X1,â€¦,Xik)unabhÃ¤ngig.\n\n[!info] #Def 2.22. UnabhÃ¤ngig und identisch verteilt\nEine Folge von Zufallsvariablen X1,X2 heisst\n\nunabhÃ¤ngig falls X1,â€¦,Xn fÃ¼r alle nâˆˆN unabhÃ¤ngig sind.\nunabhÃ¤ngig und identisch verteilt ( #short u.i.v.) ( #english independent and identically distributed ( #short i.i.d.)) falls sie unabhÃ¤ngig ist und die Zufallsvariablen dieselbe Verteilungsfunktion haben d.h. fÃ¼r alle k,lâˆˆN gilt FXk=FXl",
		"tags": ["Def", "short", "Remark", "english", "Def", "english", "short", "Theorem", "Def", "Def", "Remark", "Theorem", "Def", "short", "english", "short", "note"]
},

{
		"title": "WS L4",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/lectures/ws-l4/",
		"content": "Beweis zu Satz 2.13\n\nExistenzsatz von Kolmogorov und Folgen von i.i.d. Bernoulli Zufallsvariablen\n\n[!info] #Def 2.24 Bernoulli\nGegeben: pâˆˆ[0,1].\nEine Zufallsvariable X heisst Bernoulli Zufallsvariable mit Parameter p, wenn gilt\nP[X=0]=1âˆ’pÂ undÂ P[X=1]=pWir schreiben Xâˆ¼Ber(p).\n\n[!info] #Theorem 2.26 Existenz von Kolmogorov\nEs existiert ein Wahrscheinlichkeitsraum (Î©,F,P) und eine unendliche Folge von i.i.d. Bernoulli Zufallsvariablen X1,X2,â€¦ auf (Î©,F,P) mit Parameter 12\n\n[!info] #Theorem 2.27.\nEine Zufallsvariable U heisst gleichverteilt auf [0,1], wir schreiben Uâˆ¼U([0,1]), falls ihre Verteilungsfunktion gegeben ist durch\nFU(x)={0x&lt;0x0â‰¤xâ‰¤11x&gt;1\n\nKonstruktion von gleichverteilten Zufallsvariablen auf [0,1]\n\n[!info] #Theorem 2.28\nDie Abbildung X:Î©â†’[0,1] definiert in Gleichung (2.3) ist eine gleichverteilte Zufallsvariable auf [0,1].\n\n[!Info]- Beweis\nEs ist schnell sichtbar, dass fÃ¼r alle Ï‰âˆˆÎ© gilt, X(Ï‰)âˆˆ[0,1]. Somit gilt fÃ¼r x&lt;0, dass FX(x)=P[Xâ‰¤x]=0 und fÃ¼r xâ‰¥1, dass FX(x)=1\nSei also xâˆˆ[0,1) und sei {xn}nâˆˆN ihre eindeutige BinÃ¤rdarstellung wie in Lemma 2.29. Dann gilt\n{X&gt;x}={X1&gt;x1}âˆª{{X1=x1}âˆ©{X2&gt;x2}}âˆªâ€¦Also entweder ist die erste Ziffer grÃ¶sser oder die erste Ziffer ist gleich und die zweite ist grÃ¶sser usw.\n\n[!info] #Lemma 2.29 BinÃ¤rdarstellung\nJedes xâˆˆ[0,1) kann eindeutig in der Form\nx=âˆ‘n=1âˆ2âˆ’nxndargestellt werden, wobei fÃ¼r alle nâˆˆN gilt,xnâˆˆ{0,1}, und fÃ¼r jedes NâˆˆN gibt es ein k&gt;N, sodass xk=0 (als odie Folge &quot;endet&quot; nicht in unendlich vielen 1-en.) Die Folge {xn}nâˆˆN heisst BinÃ¤rdarstellung von x und wir schreiben x=(.x1x2â€¦)2\n\nKonstruktion von Zufallsvariablen mit beliebiger Verteilungsfunktion F\nDa wir die Gleichverteilung auf [0,1] haben, wÃ¼rden wir nun gerne Zufallsvariablen mit beliebiger Verteilungsfunktion konstruieren kÃ¶nnen.\nGegeben: F:Râ†’[0,1] eine Funktion, die die Eigenschaften (i)-(iii) aus Satz 2.13 erfÃ¼llt.\nFalls F streng monoton steigend und stetig ist, dann ist F bijektiv und es existiert eine Umkehrfunktion Fâˆ’1. FÃ¼r jedes Î±âˆˆ[0,1] ist x:=Fâˆ’1(Î±) die eindeutige reelle Zahl, fÃ¼r die F(x)=Î± gilt.\nAllgemein kann die sogenannte verallgmeinerte inverse Verteilungsfunktion oder Quantil-Funktion fÃ¼r F definiert werden.\n\n[!info] #Def 2.30 Verallgemeinerte inverse Verteilungsfunktion\nDie Verallgemeinerte inverse Verteilungsfunktion von F ist eine Abbildung Fâˆ’1:(0,1)â†’R definiert durch\nFâˆ’1(Î±)=inf{xâˆˆR|F(x)â‰¥Î±}\nNach Definition des Infimums und unter Verwendung der Rechtsstetigkeit von F gilt fÃ¼r jedes xâˆˆR und Î±âˆˆ(0,1), dass\nFâˆ’1(Î±)â‰¤xâŸºÎ±â‰¤F(x)\nMithile der verallgemeinerten inversen Verteilungsfunktion kÃ¶nnen wir nun Zufallsvariablen mit beliebigen Verteilungsfunktionen konstruieren.\n\n[!info] #Theorem 2.31. Inversionsmethode\nGegeben: F:Râ†’[0,1] eine Abbildung mit den Eigenschaften (i)-(iii) aus Satz 2.13, Uâˆ¼U([0,1])\nDann hat die Zufallsvariable X:=Fâˆ’1(U) die Verteiungsfunktion F.\n\n[!info]- Beweis\nP[Xâ‰¤x]=P[Fâˆ’1(U)â‰¤x]=P[Uâ‰¤F(x)]=F(x)â—»\n#disclaimer 2.33\nWir bemerken, dass X=Fâˆ’1(U) strenggenommen nur auf einer Menge mit Wahrscheinlichkeit 1 (da P[Uâˆˆ(0,1)]=1) aber nicht unbedingt auf ganz Î© definiert ist. Wir beheben das Problem mittels folgender Definition\nX(Ï‰){Fâˆ’1(U(Ï‰))U(Ï‰)âˆˆ(0,1)0sonstDabei spielt 0 selbst keine Rolle und es kann jede beliebe reelle Zahl genommen werden.\n\nallgemeine Folgen von unabhÃ¤ngigen Zufallsvariablen\n\n[!info] #Theorem 2.35.\nGegeben: Folge von Funktionen F1,F2 auf Râ†’[0,1], die die Eigenschaften (i)-(iii) aus Satz 2.13 erfÃ¼llen.\nDann existiert ein Wahrscheinlichkeitsraum (Î©,F,P) und eine Folge von Zufallsvariablen X1,X2,â€¦ auf diesem Wahrscheinlichkeitsraum, sodass\n\nfÃ¼r jedes k gilt, Xk hat Verteilungsfunktion Fk und\nX1,X2,â€¦ sind unabhÃ¤ngig.\nDieser Satz erlaubt es uns, direkt mit Zufallsvariablen zu arbeiten, ohne den Wahrscheinlichkeitsraum (Î©,F,P) genauer zu definieren.\nZ.B. kÃ¶nnen wir fÃ¼r zwei Verteilungsfunktionen F und G stets annehmen, dass X und Y existieren, die unabhÃ¤ngig sind und Verteilungsfunktionen F und G besitzen",
		"tags": ["Def", "Theorem", "Theorem", "Theorem", "Lemma", "Def", "Theorem", "disclaimer", "Theorem", "note"]
},

{
		"title": "WS L5",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/lectures/ws-l5/",
		"content": "[!info] #Theorem 3.3. Wahrscheinlichkeit eines Punktes\nGegeben: Zufallsvariable X:Î©â†’R mit Verteilungsfunktion F.\nFÃ¼r jedes xâˆˆR gilt P[X=x]=F(x)âˆ’F(xâˆ’)\n\nInterpretation: Sei xâˆˆR fixiert\n\nWenn F in einem Punkt xâˆˆR nicht stetig ist, dann ist die &quot;SprunghÃ¶he&quot; F(x)âˆ’F(xâˆ’) gleich der Wahrscheinlichkeit, dass X=x\nFalls F stetig in einem Punkt xâˆˆR ist, dann gilt P[X=x]=0\n\n[!info] #Def 3.4.\nGegeben: Ereignis AâˆˆF\nWir sagen A tritt P-fast sicher ( #short P-f.s.) ein, falls P[A]=1 ( #english P-almost surely ( #short P-a.s.))\n\nAbkÃ¼rzung falls Wahrscheinlichkeitsmass P klar ist und schreiben nur &quot;fast sicher ( #short f.s.)&quot;\n#disclaimer 3.5. Erweiterung der Notation auf allgemeine Mengen AâŠ‚Î© (nicht unbedingt AâˆˆF). Wir sagen, dass A fast sicher eintritt, falls ein Ereignis Aâ€²âˆˆF existiert, sodass Aâ€²âŠ‚A und P[Aâ€²]=1\ne.g. Wir schreiben Xâ‰¤Y P-f.s., falls P[Xâ‰¤Y]=1\nWenn wir mit stetigen ZV arbeiten, ist es oft restriktiv die Ungleichung X(Ï‰)â‰¤Y(Ï‰) fÃ¼r alle Ï‰âˆˆÎ© zu fordern. Deshalb fordern wir die Ungleichung nur auf einer Menge mit Mass 1.\n\nDiskrete Zufallsvariablen\n\n[!info] #Def 3.7. Diskrete Zufallsvariablen\nEine ZV X:Î©â†’R heisst diskret, falls eine endliche oder abzÃ¤hlbare Menge WâŠ‚R existiert, sodass P[XâˆˆW]=1, wenn also die Werte von X fast sicher in W liegen.\n\n#disclaimer 3.8.\ngegeben: endlicher oder abzÃ¤hlbarer Grundraum Î©\nDann ist jede ZV X:Î©â†’R diskret. In der Tat ist das Bild\n\nX(Î©)={xâˆˆR|âˆƒÏ‰âˆˆÎ©:X(Ï‰)=x}endlich oder abÃ¤hlbar und wir haben P[XâˆˆW]=1 mit W=X(Î©)\n\n[!info] #Def 3.9. Gewichtsfunktion\nFÃ¼r eine diskrete ZV X mit Wertebereich W(X)={x1,x2,â€¦} und den dazugehÃ¶rigen Wahrscheinlichkeiten {p1,p2,â€¦} definieren wir die Gewichtsfunktion ( #short probability mass function ( #short pmf)) oder diskrete Dichte von X als\npX:W(X)â†’[0,1]Â mitÂ pX(xk):=P[X=xk]=pkDie Zahlenfolge {pX(xk)}xkâˆˆW(X) nennen wir auch Verteilung von X\n\n[!info] #Proposition 3.10.\nDie Gewichtsfunktion pX einer diskreten ZV X hat folgende Eigenschaften:\n\nFÃ¼r alle xkâˆˆW(X) gilt pX(xk)âˆˆ[0,1]\nDie Wahrscheinlichkeiten addieren sich zu 1,\n\nâˆ‘xkâˆˆW(X)pX(xk)=P[XâˆˆW(X)]=1\n\n[!info] #disclaimer 3.11.\nUmgekehrt, wenn wir eine Folge von Zahlen (p(x)xâˆˆW) mit Werten in [0,1] haben, sodass âˆ‘xâˆˆWp(x)=1, dann gibt es nach Satz 2.35 einen Wahrscheinlichkeitsraum (Î©,F,P) und eine ZV X mit zugehÃ¶riger Verteilung (p(x))xâˆˆW.\nDiese Bebachtung ist in der Praxis wichtig, den sie erlaubt uns zu schreiben: &quot;Sei X eine diskrete ZV mit Verteilung (p(x))xâˆˆW.&quot;\n\nZusammenhang Verteilungs- und Gewichtsfunktion\n\n[!info] #Theorem 3.12.\nSei X eine diskrete ZV mit Werten in W und Gewichtsfunktionen pX. Dann ist die Verteilungsfunktion von X gegeben durch\nFX(x)=P[Xâ‰¤x]=âˆ‘yâ‰¤x,yâˆˆWpX(y)\nInsbesondere ist FX also durch pX vollstÃ¤ndig festgelegt. Mit dem gleichen Argument erhÃ¤lt man auch fÃ¼r jede messbare Teilmenge BâŠ†W, P[XâˆˆB]=âˆ‘xâˆˆBpX(x)\n#disclaimer 3.13\nGleichung (3.1) drÃ¼ckt die Verteilungsfunktion FX in Bezug auf pX als eine stÃ¼ckweise konstante Funktion aus.\nUmgekehrt ist eine ZV mit einer stÃ¼ckweisen konstankten Verteilungsfunktion FX diskret. W und pX sind dann gegeben durch W={Â Sprungstelle vonÂ FX} und Ã¶p(x)=\"SprunghÃ¶he im PunktÂ xâˆˆW\n\n[!info] #Theorem 3.19. Summe unabhÃ¤ngiger Bernoulli-ZV\nGegeben: pâˆˆ[0,1], nâˆˆN, unabhÃ¤ngige X1,â€¦,Xnâˆ¼Ber(p)\nDann gilt Sn:=X1+â‹¯+Xnâˆ¼Bin(n,p)\n\n[!info] #Theorem 3.23\nGegeben: unendliche Folge von unabhÃ¤ngigen Bernoulli-ZV X1,X2,â€¦ mit Parameter p\nDann ist T:=inf{nâ‰¥1|Xn=1} eine geometrisch verteilte ZV mit Parameter p.\n\n#disclaimer 3.24. Sei z.B. Tâˆ¼Geom(p), dann ist T&gt;n, wenn die ersten n Bernoulli-Expiremente fehlschlagen. Daher gilt P[T&gt;n]=(1âˆ’p)n",
		"tags": ["Theorem", "Def", "short", "english", "short", "short", "disclaimer", "Def", "disclaimer", "Def", "short", "short", "Proposition", "disclaimer", "Theorem", "disclaimer", "Theorem", "Theorem", "disclaimer", "note"]
},

{
		"title": "WS L6",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/lectures/ws-l6/",
		"content": "[!info] #Theorem 3.25 GedÃ¤chtnislosigkeit der geometrischen Verteilung\nGegeben: Tâˆ¼Geom(p), pâˆˆ(0,1)\nDann gilt fÃ¼r alle nâ‰¥0 und alle kâ‰¥1\nP[Tâ‰¥n+k|T&gt;n]=P[Tâ‰¥k]\nWenn wir also nach n Schritten immer noch auf den ersten Erfolg warten, dann ist die verbleibende Wartezeit wiederum Geom(p) verteilt.\n\n[!info]- Beweis\nP[Tâ‰¥n+k|T&gt;n]=P[{Tâ‰¥n+k}âˆ©{T&gt;n}]P[T&gt;n]=P[Tâ‰¥n+k]P[T&gt;n]=(1âˆ’p)n+kâˆ’1(1âˆ’p)n=(1âˆ’p)kâˆ’1=P[Tâ‰¥k]\n\n[!info] #Example 3.26 negativbinomiale Verteilung\nGegeben: Zufallsvariable X\nX hat eine negativbinomaile Verteilung mit Parameter râˆˆN und pâˆˆ[0,1], wir schreiben XâˆˆNBin(r,p), wenn fÃ¼r alle kâˆˆ{r,r+1,r+2,â€¦} gilt:\nP[X=k]=(kâˆ’1râˆ’1)pr(1âˆ’p)kâˆ’rDas ist eine Verallgemeinerung der geometrischen Verteilung und beschreibt die Wartezeit bis zum r -ten Erfolg in einer unendlichen Folge von Bernoulli-Experimenten. Die geometrische Verteilung erhÃ¤lt man als Spezialfall fÃ¼r r=1\n\n[!info] #Theorem 3.27\nGegeben: unendliche Folge von unabhÃ¤ngigen Bernoulli-Zufallsvariablen X1,X2 mit Parameter p\nDann hat\nTr=inf{nâ‰¥1|âˆ‘l=1nXl=r}eine negativbinomiale Verteilung mit Parametern r und p\n\n#disclaimer 3.28 Sind die Zufallsvariaben X1,â€¦,Xrâˆ¼Geom(p) und unabhÃ¤ngig, so ist ihre Summe X=X1+â‹¯+Xrâˆ¼NBin(r,p)\n\n[!info] #Example 3.29 Hypergeometrische Verteilung\nEine Zufallsvariable X heisst hypgergeometrische verteilt mit Parametern nâˆˆN und m,râˆˆ{1,â€¦,n}, wir schreiben Xâˆ¼H(n,r,m), wenn fÃ¼r alle kâˆˆ{0,1,â€¦,min(m,r)} gilt\nP[X=k]=(rk)(nâˆ’rmâˆ’k)(nm)Diese Gewichtsfunktion kommt wie folgt zustande\n#Theorem 3.30 Seien in einer Urne n GegenstÃ¤nde, davon r vom Typ 1 und nâˆ’r vom Typ 2. Es werden m GegenstÃ¤nde ohne ZurÃ¼cklegen gezogen. Sei X nun die Anzahl der gezogenen GegenstÃ¤nde vom Typ 1. Dann ist X hypergeometrisch mit den Parametern von oben verteilt.\n\n[!info]- Beweis\n\n#Example 3.31 Lotto\nIm Schweizer Lotto, wo 6 aus 42 zahlen richtig getippt werden sollen, ist die Anzahl der richtigen getippten Zahlen bei einem einzelnen Tipp hypergeometrisch verteilt mit Parametern n=42,r=6,m=6.\n\n[!info] Poisson-Verteilung\nGegeben: positive reelle Zahl Î»&gt;0\nEine Zufallsvariable X heisst Poisson-verteilt mit Parameter Î», wir schreiben Xâˆ¼Poisson(Î»), wenn sie Werte N annimt und fÃ¼r alle kâˆˆN0 gilt\nP[X=k]=Î»kk!eâˆ’Î»\n\n[!info] #Theorem 3.34 Poisson-Approximation der Binomialverteilung\nGegeben: Î»&gt;0\nFÃ¼r jedes nâ‰¥1 betrachten wir n Zufallsvariablen Xnâˆ¼Bin(n,Î»). Sei Nâˆ¼Poisson(Î»). Dann gilt fÃ¼r alle kâˆˆN\nlimnâ†’âˆP[Xn=k]=P[N=k]\n#disclaimer 3.35 Diese Art von Konvergenz wird Konvergenz in Verteilung ( #english lish convergence in distribution, convergence in law)oder schwache Konvergenz ( #english weak convergence) genannt. Intuitiv besagt sie, dass Xn und N sehr Ã¤hnliche wahrscheinlichkeitstheoretische Eigenschaften fÃ¼r grosse n haben.\n#Example 3.36\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/ws/wahrscheinlichkeit/stetige-zufallsvariablen/\">Stetige Zufallsvariablen</a>",
		"tags": ["Theorem", "Example", "Theorem", "disclaimer", "Example", "Theorem", "Example", "Theorem", "disclaimer", "english", "english", "Example", "note"]
},

{
		"title": "WS L7 Normalverteilung",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/lectures/ws-l7-normalverteilung/",
		"content": "<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/ws/wahrscheinlichkeit/normalverteilung/\">Normalverteilung</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/ws/wahrscheinlichkeit/erwartungswert/\">Erwartungswert</a>",
		"tags": [ "note"]
},

{
		"title": "WS Definitionen, Notationen",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/ws-definitionen-notationen/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Erwartungswert",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/wahrscheinlichkeit/erwartungswert/",
		"content": "[!info] #Def 4.1 Erwartungswert (nicht negativ)\nGebenen: Zufallsvariable X:Î©â†’R+ mit nicht-negativen Werten, Verteilungsfunktion FX\nDann heisst (4.1)\nE[X]=âˆ«0âˆ(1âˆ’FX(x))dxder Erwartungswert von X ( #english expected value)\n\n#disclaimer 4.2 Erwartungswert in Gleichung (4.1) ist immer definiert und kann sowohl endlich als auch unendlich werden.\n\n[!info] #Theorem 4.3\nGegeben: nicht-negative Zufallsvariable X\nDann gilt E[X]â‰¥0. Gleichheit gilt genau dann, wenn X=0 fast sicher gilt.\nFÃ¼r allgemeine Zufallsvarialben (nicht unbedingt mitkonstantem Vorzeichen) ist Erwartungswert durch Zerlegung in positiven und negativen Teil definiert.\nX+(Ï‰)={X(Ï‰)X(Ï‰)â‰¥00X(Ï‰)&lt;0undXâˆ’(Ï‰)={âˆ’X(Ï‰)X(Ï‰)â‰¤00X(Ï‰)&gt;0\nKÃ¼rzer: X+=max(X,0) und Xâˆ’=âˆ’min(X,0)=max(âˆ’X,0)\nAchtung X+ und Xâˆ’ sind nicht-negative Zufallsvariablen und es gilt\n\nX=X+âˆ’Xâˆ’\n|X|=X++Xâˆ’\n\n[!info] #Def 4.4 Allgemeiner Erwartungswert\nGegeben: reellwertige Zufallsvariable X\nFalls E[|X|]&lt;âˆ, dann heisst (4.2) E[X]=E[X+]âˆ’E[Xâˆ’] der Erwartungswert von X\n\n#disclaimer 4.5\n\nE[|X|]&lt;âˆâŸ¹E[Xâˆ’],E[X+]&lt;âˆ da |X|=X++Xâˆ’. Somit ist Gleichung (4.2) wohldefiniert.\nFÃ¼r Xâ‰¥0 ist der Erwartungswert von X immer definiert. Er kann endlich oder unendlich sein.\nWenn X kein konstantes Vorzeichen hat und E[|X|]&lt;âˆ nicht erfÃ¼llt ist, dann sagen wir, der Erwartungswert von X ist undefiniert\n\nErwartungswert aus Gleichung (4.2) kann auch wie folgt ausgedrÃ¼ckt werden E[X]=âˆ«0âˆ(1âˆ’FX(x))dxâˆ’âˆ«âˆ’âˆ0FX(x)dx\n\nErwartungswert diskreter Zufallsvariablen\n\n[!info] #Theorem 4.8 Erwartungswert (diskret)\nGegeben: diskrete Zufallsvariable X:Î©â†’R deren Werte fast sicher in W (endlich oder abzÃ¤hlbar) liegen.\nDann gilt\nE[X]=âˆ‘xâˆˆWxâ‹…P[X=x]=âˆ‘xâˆˆWxpX(x)solange der Erwartungswert wohldefiniert ist.\n\n#disclaimer 4.9 Der Erwartungswert ist wohldefiniert, sofern die Reihe âˆ‘xâˆˆWxâ‹…P[X=x] absolut konvergiert d.h. âˆ‘xâˆˆW|x|â‹…P[X=x]&lt;âˆ\nAndernfalls existiert der Erwartungswert nicht.\n\nBeispiele\n\n#Example 4.10 Bernoulli: Xâˆ¼Ber(p)âŸ¹E[X]=p\n\n#Example 4.13 Indikatorfunktion (beschreibt, ob ein Element in einer Menge ist oder nicht) 1Aâˆ¼Ber(P[A])âŸ¹E[+A]=P[A]\n\n#Example 4.12Poisson: Xâˆ¼Poisson(Î»)âŸ¹E[X]=Î»\n\n#proof\n\n#Example 4.14 Binomialverteilung: Xâˆ¼Bin(n,p)âŸ¹E[X]=np\n\n#proof\n\nErwartungswert transformierte Zufallsvariablen\n\n[!info] #Theorem 4.15\nGegeben: diskrete Zufallsvariable X:Î©â†’R, Abbildung Ï•:Râ†’R\nDann gilt $$\\mathbb{E}[\\phi(x)]=\\sum\\limits_{x\\in W}\\phi(x)\\cdot \\mathbb{P}[X=x]$$\nsolange der Erwartungswert wohldefiniert ist.",
		"tags": ["Def", "english", "disclaimer", "Theorem", "Def", "disclaimer", "Theorem", "disclaimer", "Example", "Example", "Example", "proof", "Example", "proof", "Theorem", "note"]
},

{
		"title": "Normalverteilung",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/wahrscheinlichkeit/normalverteilung/",
		"content": "[!info] #Example 3.46 Normalverteilung !\nEine stetige Zufallsvariable X heisst normalverteilt mit Parametern Î¼âˆˆR und Ïƒ2&gt;0, wir schreiben Xâˆ¼N(Î¼,Ïƒ2), falls ihre Dichte gegeben ist durch (3.5)\nfX(x)=1Ïƒ2Ï€eâˆ’12(xâˆ’Î¼Ïƒ)2\n\n#disclaimer 3.47 Die Funktion in Gleichung (3.5) hat u.a. folgende Bezeichnung: Gauss-Funktion, gausssche Normalverteilung, gausssche Verteilungsfunktion, Gauss-Kurve, (gausssche) Glockenkurve, Gauss-Glocke\n#disclaimer 3.48 Ein wichtiger Spezialfall ist die Standardnormalverteilung mit Î¼=0 und Ïƒ2=1 also N(0,1).\nDie zugehÃ¶rige dichte wird meistens mit Ï† und die Verteilungsfunktion mit Ï• bezeichnet. Die dazugehÃ¶rige Variable nennen wir meistens Z\nWeder fÃ¼r FX noch fÃ¼r Ï• gibt es einen geschlossenen Ausdruck, aber das IntegralÏ•(x)=âˆ«âˆ’âˆxÏ†(t)dt=12Ï€âˆ«âˆ’âˆxeâˆ’t22dtist tabelliert (siehe Appendix =&gt; Standardnormalverteilung)\n\nEs genÃ¼gt die Werte von Ï• zu tabellieren\n\n[!info] #Proposition 3.49 Standardnormalverteilung\nFÃ¼r Xâˆ¼N(Î¼,Ïƒ2) gilt Xâˆ’Î¼Ïƒâˆ¼N(0,1), also\nFX(x)=P[Xâ‰¤x]=P[Xâˆ’Î¼Ïƒâ‰¤xâˆ’Î¼Ïƒ]=Ï•(xâˆ’Î¼Ïƒ)\n#disclaimer 3.50\nGegeben: unabhÃ¤ngige normalverteilte Zufallsvariablen X1,â€¦,Xn mit Parametern (Î¼1,Ïƒ12),â€¦,(Î¼n,Ïƒn2). Dann gilt\n\nZ:=Î¼0+âˆ‘k=1nakXkâˆ¼N(Î¼0+âˆ‘k=1nakÎ¼k,âˆ‘k=1nak2Ïƒk2)\n#disclaimer 3.51 Aus Proposition 3.49 lesen wir direkt ab, dass fÃ¼r Î¼âˆˆR,Ïƒ2&gt;0 und Zâˆ¼N(0,1) giltÎ¼+ÏƒZâˆ¼N(Î¼,Ïƒ2)D.h. wenn wir z.B. Xâˆ¼N(Î¼,Ïƒ2) auf dem Computer simulieren wollen, reicht es ein Zâˆ¼N(0,1) zu simulieren und die erhaltenen Werte mit Ïƒ zu multiplizieren und Î¼ zu addieren.\n#disclaimer 3.52 FÃ¼r Xâˆ¼N(Î¼,Ïƒ2) liegt der &quot;Grossteil&quot; des Wahrscheinlichkeitsmasses im Intervall [Î¼âˆ’3Ïƒ,Î¼+3Ïƒ]. Genauer gilt P[|Xâˆ’Î¼|â‰¥3Ïƒ]â‰¤0.0027",
		"tags": ["Example", "disclaimer", "disclaimer", "Proposition", "disclaimer", "disclaimer", "disclaimer", "note"]
},

{
		"title": "Stetige Zufallsvariablen",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/semester4/ws/wahrscheinlichkeit/stetige-zufallsvariablen/",
		"content": "[!info] #Def 3.37 Stetig verteilte Zufallsvariablen\nEine Zufallsvariable X:Î©â†’R heisst stetig, wenn eine nicht-negative Funktion fX:Râ†’R+ existiert, sodass die Verteilungsfunktion FX dargestellt werden kann als\nFX(x)=âˆ«âˆ’âˆxfX(t)dtWir nennen fX die Dichte(-funktion) von X ( #english probability density function ( #short pdf)).\n\nIntuition: Der &quot;Wert&quot; fX(t)dt ist die Wahrscheinlichkeit, dass X Werte im Intervall [t,t+dt] annimmt.\nDie Terminologie &quot;stetig&quot; ergibt sich aus der Tatsache, dass die Gleichung oben impliziert, dass FX eine stetige Funktion ist. Insbesondere gilt fÃ¼r alle xâˆˆR laut Satz 3.3, P[X=x]=0\n\n[!info] #Def 3.38 StÃ¼ckweise stetig differenzierbare Funktion\n\nIm Kontext von R wird oft gesagt, dass ein Objekt eine Eigenschaft stÃ¼ckweise ( #english piecewise) erfÃ¼llt, wenn sie die Eigenschaft auf einer Partition des Definitionsbereichs erfÃ¼llt.\nWir sagen, eine Funktion f ist stÃ¼ckweise stetig differenzierbar, wenn es eine Partition âˆ’âˆ=x0&lt;x1&lt;â‹¯&lt;xnâˆ’1&lt;xn=âˆ gibt, sodass f auf jedem Intervall xi,xi+1 stetig differenzierbar ist.\nIntuition: D.h. dass die Funktion f nur an den Punkten x1,â€¦,xnâˆ’1 &quot;Probleme&quot; machen kÃ¶nnte. Wir lÃ¶sen diese Probleme, indem wir diese Punkte effektiv entfernen und die Funktion nur auf der daraus entstehenden Partition betrachten.\n\n[!info] #Theorem 3.39\nGegeben: Zufallsvariable X, stetige und stÃ¼ckweise differenzierbare Verteilungsfunktion FX auf einer Partition âˆ’âˆ=x0&lt;x1&lt;â‹¯&lt;xnâˆ’1&lt;xn=âˆ\nDann ist X eine stetige Zufallsvariable und die Dichtefunktion fX kann wie folgt konstruiert werden:\nfX(x){FXâ€²(x)âˆƒkâˆˆ{0,â€¦,nâˆ’1}:xâˆˆ(xk,xk+1)akxâˆˆ{x1,â€¦,xnâˆ’1}wobei die Werte ak beliebig gewÃ¤hlt werden dÃ¼rfen.\nIn anderen Worten, es gilt fX(x)=Fâ€²(X)(x) in allen Stetigkeitspunkten x von fX\n\n#disclaimer 3.40 Die Dichtefunktion fX ist (fast exakt) dast stetige Analogen zur Gewichtsfunktion pX einer diskreten Zufallsvariablen\n\nFÃ¼r messbare Mengen BâŠ‚R giltP[XâˆˆB]=âˆ«BfX(x)dxÂ analog zuÂ P[XâˆˆB]=âˆ‘xâˆˆBpX(x)\nDa allerdings P[X=x]=0 fÃ¼r jedes xâˆˆR, hat in diesem Sinn X keine Gewichtsfunktion wie eine diskrete Zufallsvariable.\nIst allerdings fX stetig an der Stelle x, so haben wirfX)(x)=limÏµâ†˜01Ïµâˆ«xx+ÏµfX(t)dt=limÏµâ†˜0P[x&lt;Xâ‰¤x+Ïµ]Ïµund somit\nP[Xâˆˆ(x,x+Ïµ)]â‰ˆÏµfX(x) fÃ¼r kleine Ïµ\nEinfache Merkregel: Um vom diskreten zum stetigen Fall zu kommen, kann die Kombination (Summe, Gewichtsfunktion) systematisch durch (Integral, Dichtefunktion) ersetzt werden.\n\nBeispiele\n\n#Example 3.41 Gleichverteilung:\n\nIntuition: Die Gleichverteilung auf einem Intervall [a,b] ist ein Modell fÃ¼r die zufÃ¤llige Wahl eines Punktes in [a,b].\nEigenschaften:\n\n#Example 3.43 Exponentialverteilung\n\nIntuition:\nEigenschaften:\n\n#Example 3.45 Cauchy-Verteilung:\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/semester4/ws/wahrscheinlichkeit/normalverteilung/\">Normalverteilung</a>",
		"tags": ["Def", "english", "short", "Def", "english", "Theorem", "disclaimer", "Example", "Example", "Example", "note"]
},

{
		"title": "home page",
		"date":"Tue Apr 16 2024 21:45:20 GMT+0000 (Coordinated Universal Time)",
		"url":"/",
		"content": "",
		"tags": [ "note","gardenEntry"]
}
]